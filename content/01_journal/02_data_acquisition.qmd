---
title: "Tidyverse"
author: "Nis Köpke"
---

# Data Acquisition Preparation
(Code mainly from startupengineer templates)

Business code commented out because of website changes.

```{r}
# WEBSCRAPING ----

# 1.0 LIBRARIES ----

library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
library(RSQLite)
library(httr)

# Import and list tables
con <- RSQLite::dbConnect(drv = SQLite(), dbname = "./../../00_data/02_chinook/Chinook_Sqlite.sqlite")
dbListTables(con)

# Look at a table
tbl(con, "Album")

# Save data to memory
album_tbl <- tbl(con, "Album") %>% collect()

# Close database connection
dbDisconnect(con)
con

# glue string interpolation example
name <- "Fred"
glue('My name is {name}.')

# API GET example
resp <- GET("https://swapi.dev/api/people/1/")

# Wrapped into a function
sw_api <- function(path) {
  url <- modify_url(url = "https://swapi.dev", path = glue("/api{path}"))
  resp <- GET(url)
  stop_for_status(resp) # automatically throws an error if a request did not succeed
}

# API GET function example
resp <- sw_api("/people/1")
resp

# convert API response body
resp_extracted <- rawToChar(resp$content)  %>% fromJSON()
resp_extracted

# Test list
data_list <- list(strings= c("string1", "string2"), 
                  numbers = c(1,2,3), 
                  TRUE, 
                  100.23, 
                  tibble(
                    A = c(1,2), 
                    B = c("x", "y")
                  )
)

# access list
resp %>% 
  .$content %>% 
  rawToChar() %>% 
  fromJSON()

# directly parse body
# content(resp, as = "text")
# content(resp, as = "parsed")
content(resp)

# call to alpha vantage api
resp <- GET('https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol=WDI.DE')
resp

# test the renviron file
userid <- Sys.getenv('userid')
pwd <- Sys.getenv('pwd')
key <- Sys.getenv('key')

# web scraping example
# get the URL for the wikipedia page with all S&P 500 symbols
url <- "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
# use that URL to scrape the S&P 500 table using rvest
library(rvest)
sp_500 <- url %>%
  # read the HTML from the webpage
  read_html() %>%
  # Get the nodes with the id
  html_nodes(css = "#constituents") %>%
  # html_nodes(xpath = "//*[@id='constituents']"") %>% 
  # Extract the table and turn the list into a tibble
  html_table() %>% 
  .[[1]] %>% 
  as_tibble()

# web scraping example 2
url  <- "https://www.imdb.com/chart/top/?ref_=nv_mv_250"
html <- url %>% 
  read_html

# get the ranks
rank <-  html %>% 
  html_nodes(css = ".titleColumn") %>% 
  html_text() %>% 
  # Extrag all digits between " " and ".\n" The "\" have to be escaped
  # You can use Look ahead "<=" and Look behind "?=" for this
  stringr::str_extract("(?<= )[0-9]*(?=\\.\\n)")%>% 
  # Make all values numeric
  as.numeric()

# get the titles
title <- html %>% 
  html_nodes(".titleColumn > a") %>% 
  html_text()

# get the year
year <- html %>% 
  html_nodes(".titleColumn .secondaryInfo") %>%
  html_text() %>% 
  # Extract numbers
  stringr::str_extract(pattern = "[0-9]+") %>% 
  as.numeric()

# get the people
people <- html %>% 
  html_nodes(".titleColumn > a") %>% 
  html_attr("title")

# get the ratings
rating <- html %>% 
  html_nodes(css = ".imdbRating > strong") %>% 
  html_text() %>% 
  as.numeric()

# get the number of ratings
num_ratings <- html %>% 
  html_nodes(css = ".imdbRating > strong") %>% 
  html_attr('title') %>% 
  # Extract the numbers and remove the comma to make it numeric values
  stringr::str_extract("(?<=based on ).*(?=\ user ratings)" ) %>% 
  stringr::str_replace_all(pattern = ",", replacement = "") %>% 
  as.numeric()

# merge everything
imdb_tbl <- tibble(rank, title, year, people, rating, num_ratings)

# # read bike data from json file
# bike_data_lst <- fromJSON("bike_data.json")
# # Open the data by clicking on it in the environment or by running View()
# View(bike_data_lst)
# 
# # color path
# # productDetail --> variationAttributes --> values --> [[1]] --> displayValue
# bike_data_lst[["productDetail"]][["variationAttributes"]][["values"]][[1]][["displayValue"]]
# 
# # color extraction using pluck
# bike_data_lst %>%
#   purrr::pluck("productDetail", "variationAttributes", "values", 1, "displayValue")

# Scraping example
  # # 1.1 collect product families ----
  # 
  # url_home          <- "https://www.canyon.com/en-de"
  # xopen(url_home) # Open links directly from RStudio to inspect them
  # 
  # # Read in the HTML for the entire webpage
  # html_home         <- read_html(url_home)
  # 
  # # Web scrape the ids for the families
  # bike_family_tbl <- html_home %>%
  #   
  #   # Get the nodes for the families ...
  #   html_nodes(css = ".js-navigationDrawer__list--secondary") %>%
  #   # ...and extract the information of the id attribute
  #   html_attr('id') %>%
  #   
  #   # Remove the product families Gear and Outlet and Woman 
  #   # (because the female bikes are also listed with the others)
  #   discard(.p = ~stringr::str_detect(.x,"WMN|WOMEN|GEAR|OUTLET")) %>%
  #   
  #   # Convert vector to tibble
  #   enframe(name = "position", value = "family_class") %>%
  #   
  #   # Add a hashtag so we can get nodes of the categories by id (#)
  #   mutate(
  #     family_id = str_glue("#{family_class}")
  #   )
  # 
  # bike_family_tbl
  # 
  # # 1.2 COLLECT PRODUCT CATEGORIES ----
  # 
  # # Combine all Ids to one string so that we will get all nodes at once
  # # (seperated by the OR operator ",")
  # family_id_css <- bike_family_tbl %>%
  #   pull(family_id) %>%
  #   stringr::str_c(collapse = ", ")
  # family_id_css
  # ## "#js-navigationList-ROAD, #js-navigationList-MOUNTAIN, #js-navigationList-EBIKES, #js-navigationList-HYBRID-CITY, #js-navigationList-YOUNGHEROES"
  # 
  # # Extract the urls from the href attribute
  # bike_category_tbl <- html_home %>%
  #   
  #   # Select nodes by the ids
  #   html_nodes(css = family_id_css) %>%
  #   
  #   # Going further down the tree and select nodes by class
  #   # Selecting two classes makes it specific enough
  #   html_nodes(css = ".navigationListSecondary__listItem .js-ridestyles") %>%
  #   html_attr('href') %>%
  #   
  #   # Convert vector to tibble
  #   enframe(name = "position", value = "subdirectory") %>%
  #   
  #   # Add the domain, because we will get only the subdirectories
  #   mutate(
  #     url = glue("https://www.canyon.com{subdirectory}")
  #   ) %>%
  #   
  #   # Some categories are listed multiple times.
  #   # We only need unique values
  #   distinct(url)
  # 
  # bike_category_tbl
  # 
  # # 2.0 COLLECT BIKE DATA ----
  # 
  # # 2.1 Get URL for each bike of the Product categories
  # 
  # # select first bike category url
  # bike_category_url <- bike_category_tbl$url[1]
  # 
  # # Alternatives for selecting values
  # # bike_category_url <- bike_category_tbl %$% url %>% .[1]
  # # bike_category_url <- bike_category_tbl %>% pull(url) %>% .[1]
  # # bike_category_url <- deframe(bike_category_tbl[1,])
  # # bike_category_url <- bike_category_tbl %>% first %>% first
  # 
  # xopen(bike_category_url)
  # 
  # # Get the URLs for the bikes of the first category
  # html_bike_category  <- read_html(bike_category_url)
  # bike_url_tbl        <- html_bike_category %>%
  #   
  #   # Get the 'a' nodes, which are hierarchally underneath 
  #   # the class productTile__contentWrapper
  #   html_nodes(css = ".productTile__contentWrapper > a") %>%
  #   html_attr("href") %>%
  #   
  #   # Remove the query parameters of the URL (everything after the '?')
  #   str_remove(pattern = "\\?.*") %>%
  #   
  #   # Convert vector to tibble
  #   enframe(name = "position", value = "url")
  # 
  # # 2.1.2 Extract the descriptions (since we have retrieved the data already)
  # bike_desc_tbl <- html_bike_category %>%
  #   
  #   # Get the nodes in the meta tag where the attribute itemprop equals description
  #   html_nodes('.productTile__productSummaryLeft > meta[itemprop="description"]') %>%
  #   
  #   # Extract the content of the attribute content
  #   html_attr("content") %>%
  #   
  #   # Convert vector to tibble
  #   enframe(name = "position", value = "description")
  # 
  # # 2.1.3 Get even more data from JSON files
  # bike_json_tbl  <- html_bike_category %>%
  #   
  #   html_nodes(css = '.productGrid__listItem.xlt-producttile > div') %>%
  #   html_attr("data-gtm-impression") %>%
  #   
  #   # Convert the JSON format to dataframe
  #   # map runs that function on each element of the list
  #   map(fromJSON) %>% # need JSON ### need lists
  #   
  #   # Extract relevant information of the nested list
  #   map(purrr::pluck, 2, "impressions") %>% # Need purrr and expl above
  #   
  #   # Set "not defined" and emtpy fields to NA (will be easier to work with)
  #   map(na_if, "not defined") %>%
  #   map(na_if, "") %>%
  #   
  #   # The class of dimension56 and price varies between numeric and char.
  #   # This converts this column in each list to numeric
  #   # across allows to perform the same operation on multiple columns
  #   map(~mutate(., across(c("dimension56","price"), as.numeric))) %>%
  #   
  #   # Stack all lists together
  #   bind_rows() %>%
  #   # Convert to tibble so that we have the same data format
  #   as_tibble() %>%
  #   
  #   # Add consecutive numbers so that we can bind all data together
  #   # You could have also just use bind_cols()
  #   rowid_to_column(var='position') %>%
  #   left_join(bike_desc_tbl) %>%
  #   left_join(bike_url_tbl)
  # 
  # # 2.2 Wrap it into a function ----
  # get_bike_data <- function(url) {
  #   
  #   html_bike_category <- read_html(url)
  #   
  #   # Get the URLs
  #   bike_url_tbl  <- html_bike_category %>%
  #     html_nodes(css = ".productTile__contentWrapper > a") %>%
  #     html_attr("href") %>%
  #     str_remove(pattern = "\\?.*") %>%
  #     enframe(name = "position", value = "url")
  #   
  #   # Get the descriptions
  #   bike_desc_tbl <- html_bike_category %>%
  #     html_nodes(css = '.productTile__productSummaryLeft > 
  #                       meta[itemprop="description"]') %>%
  #     html_attr("content") %>%
  #     enframe(name = "position", value = "description")
  #   
  #   # Get JSON data
  #   bike_json_tbl <- html_bike_category %>%
  #     html_nodes(css = '.productGrid__listItem.xlt-producttile > div') %>%
  #     html_attr("data-gtm-impression") %>%
  #     map(fromJSON) %>% # need JSON ### need lists
  #     map(purrr::pluck, 2, "impressions") %>% 
  #     map(na_if, "not defined") %>%
  #     map(na_if, "") %>%
  #     map(~mutate(., across(c("dimension56","price"), as.numeric))) %>%
  #     bind_rows() %>%
  #     as_tibble() %>%
  #     rowid_to_column(var='position') %>%
  #     left_join(bike_desc_tbl) %>%
  #     left_join(bike_url_tbl)
  # }
  # 
  # # Run the function with the first url to check if it is working
  # bike_category_url <- bike_category_tbl$url[1]
  # bike_data_tbl     <- get_bike_data(url = bike_category_url)
  # 
  # bike_data_tbl
  # 
  # # 2.3.1a Map the function against all urls
  # 
  # # Extract the urls as a character vector
  # bike_category_url_vec <- bike_category_tbl %>% 
  #   pull(url)
  # 
  # # Run the function with every url as an argument
  # bike_data_lst <- map(bike_category_url_vec, get_bike_data)
  # 
  # # Merge the list into a tibble
  # bike_data_tbl <- bind_rows(bike_data_lst)
  # saveRDS(bike_data_tbl, "bike_data_tbl.rds")
  # 
  # # 2.3.1b Alternative with a for loop
  # 
  # # Create an empty tibble, that we can populate
  # bike_data_tbl <- tibble()
  # 
  # # Loop through all urls
  # for (i in seq_along(bike_category_tbl$url)) {
  #   
  #   bike_category_url <- bike_category_tbl$url[i]
  #   bike_data_tbl     <- bind_rows(bike_data_tbl, get_bike_data(bike_category_url))
  #   
  #   # Wait between each request to reduce the load on the server 
  #   # Otherwise we could get blocked
  #   Sys.sleep(5)
  #   
  #   # print the progress
  #   print(i)
  #   
  # }
  # 
  # # Check for duplicates
  # bike_data_tbl %>%
  #   group_by(id) %>%
  #   filter(n()>1) %>%
  #   arrange(id) %>% 
  #   View()
  # 
  # # Filter non Canyon bikes (based on id length) and add an empty column for the colors
  # bike_data_cleaned_tbl <- bike_data_tbl %>%
  #   
  #   # Filter for bikes. Only unique ones
  #   filter(nchar(.$id) == 4) %>%
  #   filter(!(name %>% str_detect("Frameset"))) %>%
  #   distinct(id, .keep_all = T) %>%
  #   
  #   # Split categories (Speedmax had to be treated individually)
  #   mutate(category = replace(category, 
  #                             name == "Speedmax CF SLX 8.0 SL", "Road/Triathlon Bike/Speedmax")) %>%
  #   separate(col = category, into = c("category_1",
  #                                     "category_2",
  #                                     "category_3"),
  #            sep = "(?<!\\s)/(?!\\s)") %>%
  #   
  #   # Renaming
  #   rename("year"       = "dimension50") %>%
  #   rename("model"      = "name") %>%
  #   rename("gender"     = "dimension63") %>%
  #   rename("price_euro" = "metric4") %>%
  #   
  #   # Fix years manually (have checked the website)
  #   mutate(year = replace_na(year, 2021)) %>%
  #   
  #   # Add frame material
  #   mutate(frame_material = case_when(
  #     model %>% str_detect(" CF ") ~ "carbon",
  #     model %>% str_detect(" CFR ") ~ "carbon",
  #     TRUE ~ "aluminium"
  #   )
  #   ) %>%
  #   
  #   # Select and order columns
  #   select(-c(position, brand, variant, starts_with("dim"), 
  #             quantity, feedProductId, price, metric5)) %>%
  #   select(id, model, year, frame_material, price_euro, everything())
  # 
  # saveRDS(bike_data_cleaned_tbl, "bike_data_cleaned_tbl.rds")
  # 
  # # 3.1a Get all color variations for each bike
  # 
  # # Extract all bike urls
  # bike_url_vec <- bike_data_cleaned_tbl %>% 
  #   pull(url)
  # 
  # # Create function to get the variations
  # get_colors <- function(url) {
  #   
  #   url %>%
  #     
  #     read_html() %>%
  #     
  #     # Get all 'script nodes' and convert to char
  #     html_nodes(css = "script") %>%
  #     as.character() %>%
  #     
  #     # Select the node, that contains 'window.deptsfra'
  #     str_subset(pattern = "window.deptsfra") %>%
  #     
  #     # remove the chars that do not belong to the json
  #     # 1. replace at the beginning everything until the first "{" with ""
  #     str_replace("^[^\\{]+", "") %>%
  #     # 2. replace at the end everything after the last "}" with ""
  #     str_replace("[^\\}]+$", "") %>%
  #     
  #     # Convert from json to an r object and pick the relevant values
  #     fromJSON() %>%
  #     purrr::pluck("productDetail", "variationAttributes", "values", 1, "value")
  # }
  # 
  # # Run the function over all urls and add result to bike_data_cleaned_tbl
  # # This will take a long time (~ 20-30 minutes) because we have to iterate over many bikes
  # bike_data_colors_tbl <- bike_data_cleaned_tbl %>% 
  #   mutate(colors = map(bike_url_vec, get_colors))
  # 
  # saveRDS(bike_data_colors_tbl, "bike_data_colors_tbl.rds")
  # 
  # library(furrr)     # Parallel Processing using purrr (iteration)
  # plan("multiprocess")
  # bike_data_colors_tbl <- bike_data_cleaned_tbl %>% 
  #   mutate(colors = future_map(bike_url_vec, get_colors))
  # 
  # # 3.2 Create the urls for each variation
  # 
  # bike_data_colors_tbl <- bike_data_colors_tbl %>%
  #   
  #   # Create entry for each color variation
  #   unnest(colors) %>%
  #   
  #   # Merge url and query parameters for the colors
  #   mutate(url_color = glue("{url}?dwvar_{id}_pv_rahmenfarbe={colors}")) %>%
  #   select(-url) %>%
  #   
  #   # Use stringi to replace the last dash with the HTLM format of a dash (%2F)
  #   # Only if there is a dash in the color column
  #   mutate(url_color = ifelse(str_detect(colors, pattern = "/"),
  #                             
  #                             # if TRUE --> replace      
  #                             stringi::stri_replace_last_fixed(url_color, "/", "%2F"),
  #                             
  #                             # ELSE --> take the original url
  #                             url_color))
  # 
  # bike_data_colors_tbl %>% glimpse()
  # 
  # # Create function
  # get_sizes <- function(url) {
  #   
  #   json <- url %>%
  #     
  #     read_html() %>%
  #     
  #     # Get all 'script nodes' and convert to char
  #     html_nodes(css = "script") %>%
  #     as.character() %>%
  #     
  #     # Select the node, that contains 'window.deptsfra'
  #     str_subset(pattern = "window.deptsfra") %>%
  #     
  #     # remove the chars that do not belong to the json
  #     # 1. replace at the beginning everything until the first "{" with ""
  #     str_replace("^[^\\{]+", "") %>%
  #     # 2. replace at the end everything after the last "}" with ""
  #     str_replace("[^\\}]+$", "") %>%
  #     
  #     # Convert from json to an r object and pick the relevant values
  #     fromJSON(flatten = T) %>%
  #     purrr::pluck("productDetail", "variationAttributes", "values", 2) %>%
  #     
  #     # select(id, value, available, availability)# %>%
  #     select(id, value, availability.onlyXLeftNumber) %>%
  #     
  #     # Rename
  #     rename(id_size = id) %>%
  #     rename(size = value) %>%
  #     rename(stock_availability = availability.onlyXLeftNumber) %>%
  #     
  #     # Conver to tibble
  #     as_tibble()
  #   
  # }
  # 
  # # Pull url vector
  # bike_url_color_vec <- bike_data_colors_tbl %>% 
  #   pull(url_color)
  # 
  # # Map
  # bike_data_sizes_tbl <- bike_data_colors_tbl %>% 
  #   mutate(size = future_map(bike_url_color_vec, get_sizes))
  # 
  # # Unnest
  # bike_data_sizes_tbl <- bike_data_sizes_tbl %>% 
  #   unnest(size)
  # 
  # saveRDS(bike_data_sizes_tbl, "bike_data_sizes_tbl.rds")
```

# Data Acquisition Challenge 1

Data is requested from openweathermap, some data is extracted and the result is visualized.

## API call and data extraction


```{r}
# Challenge 2.1 ----

# 1.0 LIBRARIES ----

library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
library(RSQLite)
library(httr)

# Get api key from renviron file 
apikey <- Sys.getenv('key')

# store openweathermap city id for hamburg to variable
city_id <- 2911298

# Call to openweathermao api 
weather <- GET(glue("http://api.openweathermap.org/data/2.5/forecast?id={city_id}&APPID={apikey}"))

# convert API response body
weather_extracted <- rawToChar(weather$content)  %>% fromJSON()
weather_extracted

# extract wind speed data from response
windspeed <- weather_extracted$list$wind$speed
windspeed  %>% glimpse()

# extract city name from response
city_name <- weather_extracted$city$name

# hour list
hours <- seq(0,120-1,3)

# combine hour list and windspeed data
windspeed_tbl <- tibble(hours, windspeed)
windspeed_tbl %>% glimpse()
```

## Visualization

```{r}
# plot results
windspeed_tbl %>%
  
  # Set up x, y, fill
  ggplot(aes(x = hours, y = windspeed)) +
  
  # Geometries
  geom_col() + # Run up to here to get a stacked bar plot
  
  # Formatting
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = "m/s")) +
  labs(
    title = glue("Wind speed in {city_name}"),
    subtitle = "Forecast for the next 5 days in 3 hour intervals",
    x = "Nr. of hours into the future", 
    y = "Wind speed in m/s")
```

# Data Acquisition Challenge 2

```{r}
# Challenge 2.2 ----

# 1.0 LIBRARIES ----

library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
library(RSQLite)
library(httr)
library(xml2)

# URL of shop
base_url <- "https://www.rosebikes.com"

# Collecting info about bikes of MTB category
mtb_url = glue("{base_url}/bikes/mtb")

# Open mtb url
# xopen(mtb_url)

# create table and extract href for model from html
mtb_tbl <-  read_html(mtb_url) %>% 
  html_nodes(css = ".catalog-category-bikes__picture-wrapper") %>% 
  map(xml_attrs) %>% 
  map("href") %>% 
  unlist() %>% 
  as_tibble() %>% 
  rename("modelurl" = value) %>%
  # Add the domain, because we will get only the subdirectories
  mutate(modelurl = glue("{base_url}{modelurl}"))  %>%
  
  # Some categories are listed multiple times.
  # We only need unique values
  distinct(modelurl)

# extract href for bikes of html for every url in mtb_tbl
bike_tbl <- mtb_tbl$modelurl %>% 
  map(read_html) %>% 
  map(html_nodes, css = ".catalog-category-model__picture-link") %>% 
  map(xml_attrs) 

# flatten nested lists
bike_tbl <- unlist(bike_tbl, recursive=FALSE)

# extract href from all bikes
bike_tbl <- bike_tbl %>% 
  map("href") %>% 
  unlist() %>% 
  as_tibble() %>% 
  rename("bikeurl" = value) %>%
  mutate(bikeurl = glue("{base_url}{bikeurl}"))  %>%
 
  # Some categories are listed multiple times.
  # We only need unique values
  distinct(bikeurl)


# removing entries that give errors
bike_tbl <- bike_tbl[-8,]
bike_tbl <- bike_tbl[-52,]

# save first entries of bike_tbl to variable for testing
# bike_tbl <- bike_tbl[1:4,]  %>%   as_tibble()

price_tbl <- bike_tbl$bikeurl %>% 
  map(read_html) %>%
  map(html_nodes, css = ".detail-price__wrapper > span") %>% 
  map(xml_attrs)

# flatten nested lists
price_tbl <- unlist(price_tbl, recursive=FALSE)

# extract price from all entries
price_tbl <- price_tbl %>% 
  map("data-test") %>% 
  unlist() %>% 
  as_tibble() %>% 
  rename("price" = value)

# remove dollar signs from price 
price_tbl$price <- price_tbl$price %>% 
  str_remove_all("€") %>% 
  str_remove_all("\\.") %>% 
  as.numeric()

name_tbl <- bike_tbl$bikeurl %>% 
  map(read_html) %>%
  map(html_nodes, ".basic-headline__title") %>% 
  map(xml_find_all, ".//text()")

# for loop through name_tbl to get only the text 
for (i in 1:length(name_tbl)) {
  name_tbl[[i]] <- as_list(name_tbl[[i]])[[1]]
}

name_tbl <- name_tbl %>% 
  unlist() %>% 
  as_tibble() %>% 
  rename("name" = value)

# combine rows from name_tbl and price_tbl
combi_tbl <- bind_cols(name_tbl, price_tbl)

# preview first 10 rows
combi_tbl %>% head(10)
```

Note the duplicates in the names are due to different color options which are not yet included.

