[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "1 Data Wrangling Preparation\n(Code mainly from startupengineer templates)\n\n# Data wrangling ----\n\n# 1.0 LIBRARIES ----\n\n# Tidyverse\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(vroom)\n\n#> \n#> Attaching package: 'vroom'\n#> \n#> The following objects are masked from 'package:readr':\n#> \n#>     as.col_spec, col_character, col_date, col_datetime, col_double,\n#>     col_factor, col_guess, col_integer, col_logical, col_number,\n#>     col_skip, col_time, cols, cols_condense, cols_only, date_names,\n#>     date_names_lang, date_names_langs, default_locale, fwf_cols,\n#>     fwf_empty, fwf_positions, fwf_widths, locale, output_column,\n#>     problems, spec\n\n# Data Table\nlibrary(data.table)\n\n#> \n#> Attaching package: 'data.table'\n#> \n#> The following objects are masked from 'package:lubridate':\n#> \n#>     hour, isoweek, mday, minute, month, quarter, second, wday, week,\n#>     yday, year\n#> \n#> The following objects are masked from 'package:dplyr':\n#> \n#>     between, first, last\n#> \n#> The following object is masked from 'package:purrr':\n#> \n#>     transpose\n\n# 2.0 DATA IMPORT ----\n\n# 2.1 Loan Acquisitions Data ----\ncol_types_acq <- list(\n  loan_id                            = col_factor(),\n  original_channel                   = col_factor(NULL),\n  seller_name                        = col_factor(NULL),\n  original_interest_rate             = col_double(),\n  original_upb                       = col_integer(),\n  original_loan_term                 = col_integer(),\n  original_date                      = col_date(\"%m/%Y\"),\n  first_pay_date                     = col_date(\"%m/%Y\"),\n  original_ltv                       = col_double(),\n  original_cltv                      = col_double(),\n  number_of_borrowers                = col_double(),\n  original_dti                       = col_double(),\n  original_borrower_credit_score     = col_double(),\n  first_time_home_buyer              = col_factor(NULL),\n  loan_purpose                       = col_factor(NULL),\n  property_type                      = col_factor(NULL),\n  number_of_units                    = col_integer(),\n  occupancy_status                   = col_factor(NULL),\n  property_state                     = col_factor(NULL),\n  zip                                = col_integer(),\n  primary_mortgage_insurance_percent = col_double(),\n  product_type                       = col_factor(NULL),\n  original_coborrower_credit_score   = col_double(),\n  mortgage_insurance_type            = col_double(),\n  relocation_mortgage_indicator      = col_factor(NULL))\n\nacquisition_data <- vroom(\n      file       = \"./../../loan_data/Acquisition_2019Q1.txt\", \n      delim      = \"|\", \n      col_names  = names(col_types_acq),\n      col_types  = col_types_acq,\n      na         = c(\"\", \"NA\", \"NULL\"))\n\nacquisition_data %>% glimpse()\n\n#> Rows: 297,452\n#> Columns: 25\n#> $ loan_id                            <fct> 100000913397, 100017539727, 1000180…\n#> $ original_channel                   <fct> C, B, R, C, B, C, R, R, R, B, R, C,…\n#> $ seller_name                        <fct> \"JPMORGAN CHASE BANK, NATIONAL ASSO…\n#> $ original_interest_rate             <dbl> 5.875, 4.750, 4.875, 4.875, 4.250, …\n#> $ original_upb                       <int> 324000, 307000, 256000, 248000, 490…\n#> $ original_loan_term                 <int> 360, 360, 360, 360, 360, 360, 360, …\n#> $ original_date                      <date> 2018-09-01, 2018-12-01, 2018-11-01…\n#> $ first_pay_date                     <date> 2018-11-01, 2019-02-01, 2019-01-01…\n#> $ original_ltv                       <dbl> 80, 90, 90, 90, 67, 69, 95, 80, 97,…\n#> $ original_cltv                      <dbl> 80, 90, 90, 90, 67, 69, 95, 80, 97,…\n#> $ number_of_borrowers                <dbl> 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1,…\n#> $ original_dti                       <dbl> 49, 44, 41, 40, 35, 31, 45, 47, 37,…\n#> $ original_borrower_credit_score     <dbl> 692, 722, 728, 730, 727, 798, 710, …\n#> $ first_time_home_buyer              <fct> N, N, N, Y, Y, N, N, N, Y, Y, N, N,…\n#> $ loan_purpose                       <fct> C, P, P, P, P, P, P, P, P, P, P, P,…\n#> $ property_type                      <fct> PU, PU, SF, SF, CO, PU, PU, CO, SF,…\n#> $ number_of_units                    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n#> $ occupancy_status                   <fct> P, P, S, P, P, P, P, P, P, P, P, P,…\n#> $ property_state                     <fct> CA, TX, NC, IL, CA, FL, TX, TX, IN,…\n#> $ zip                                <int> 925, 770, 286, 600, 945, 337, 773, …\n#> $ primary_mortgage_insurance_percent <dbl> NA, 25, 25, 25, NA, NA, 30, NA, 35,…\n#> $ product_type                       <fct> FRM, FRM, FRM, FRM, FRM, FRM, FRM, …\n#> $ original_coborrower_credit_score   <dbl> 665, NA, 738, 791, NA, 810, 778, NA…\n#> $ mortgage_insurance_type            <dbl> NA, 1, 1, 1, NA, NA, 1, NA, 1, 1, 1…\n#> $ relocation_mortgage_indicator      <fct> N, N, N, N, N, N, Y, N, N, N, N, N,…\n\n# 2.2 Performance Data ----\ncol_types_perf = list(\n    loan_id                                = col_factor(),\n    monthly_reporting_period               = col_date(\"%m/%d/%Y\"),\n    servicer_name                          = col_factor(NULL),\n    current_interest_rate                  = col_double(),\n    current_upb                            = col_double(),\n    loan_age                               = col_double(),\n    remaining_months_to_legal_maturity     = col_double(),\n    adj_remaining_months_to_maturity       = col_double(),\n    maturity_date                          = col_date(\"%m/%Y\"),\n    msa                                    = col_double(),\n    current_loan_delinquency_status        = col_double(),\n    modification_flag                      = col_factor(NULL),\n    zero_balance_code                      = col_factor(NULL),\n    zero_balance_effective_date            = col_date(\"%m/%Y\"),\n    last_paid_installment_date             = col_date(\"%m/%d/%Y\"),\n    foreclosed_after                       = col_date(\"%m/%d/%Y\"),\n    disposition_date                       = col_date(\"%m/%d/%Y\"),\n    foreclosure_costs                      = col_double(),\n    prop_preservation_and_repair_costs     = col_double(),\n    asset_recovery_costs                   = col_double(),\n    misc_holding_expenses                  = col_double(),\n    holding_taxes                          = col_double(),\n    net_sale_proceeds                      = col_double(),\n    credit_enhancement_proceeds            = col_double(),\n    repurchase_make_whole_proceeds         = col_double(),\n    other_foreclosure_proceeds             = col_double(),\n    non_interest_bearing_upb               = col_double(),\n    principal_forgiveness_upb              = col_double(),\n    repurchase_make_whole_proceeds_flag    = col_factor(NULL),\n    foreclosure_principal_write_off_amount = col_double(),\n    servicing_activity_indicator           = col_factor(NULL))\n\nperformance_data <- vroom(\n    file       = \"./../../loan_data/Performance_2019Q1.txt\", \n    delim      = \"|\", \n    col_names  = names(col_types_perf),\n    col_types  = col_types_perf,\n    na         = c(\"\", \"NA\", \"NULL\"))\n\nperformance_data %>% glimpse()\n\n#> Rows: 3,105,039\n#> Columns: 31\n#> $ loan_id                                <fct> 100000913397, 100000913397, 100…\n#> $ monthly_reporting_period               <date> 2019-01-01, 2019-02-01, 2019-0…\n#> $ servicer_name                          <fct> \"OTHER\", NA, NA, NA, NA, NA, NA…\n#> $ current_interest_rate                  <dbl> 5.875, 5.875, 5.875, 5.875, 5.8…\n#> $ current_upb                            <dbl> NA, NA, NA, NA, NA, NA, 320968.…\n#> $ loan_age                               <dbl> 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…\n#> $ remaining_months_to_legal_maturity     <dbl> 357, 356, 355, 354, 353, 352, 3…\n#> $ adj_remaining_months_to_maturity       <dbl> 357, 356, 355, 354, 353, 352, 3…\n#> $ maturity_date                          <date> 2048-10-01, 2048-10-01, 2048-1…\n#> $ msa                                    <dbl> 40140, 40140, 40140, 40140, 401…\n#> $ current_loan_delinquency_status        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ modification_flag                      <fct> N, N, N, N, N, N, N, N, N, N, N…\n#> $ zero_balance_code                      <fct> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ zero_balance_effective_date            <date> NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ last_paid_installment_date             <date> NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ foreclosed_after                       <date> NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ disposition_date                       <date> NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ foreclosure_costs                      <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ prop_preservation_and_repair_costs     <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ asset_recovery_costs                   <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ misc_holding_expenses                  <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ holding_taxes                          <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ net_sale_proceeds                      <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ credit_enhancement_proceeds            <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ repurchase_make_whole_proceeds         <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ other_foreclosure_proceeds             <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ non_interest_bearing_upb               <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ principal_forgiveness_upb              <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ repurchase_make_whole_proceeds_flag    <fct> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ foreclosure_principal_write_off_amount <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ servicing_activity_indicator           <fct> N, N, N, N, N, N, N, N, N, N, N…\n\n# 3.1 Acquisition Data ----\nclass(acquisition_data)\n\n#> [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\"\n\nsetDT(acquisition_data)\n\nclass(acquisition_data)\n\n#> [1] \"data.table\" \"data.frame\"\n\nacquisition_data %>% glimpse()\n\n#> Rows: 297,452\n#> Columns: 25\n#> $ loan_id                            <fct> 100000913397, 100017539727, 1000180…\n#> $ original_channel                   <fct> C, B, R, C, B, C, R, R, R, B, R, C,…\n#> $ seller_name                        <fct> \"JPMORGAN CHASE BANK, NATIONAL ASSO…\n#> $ original_interest_rate             <dbl> 5.875, 4.750, 4.875, 4.875, 4.250, …\n#> $ original_upb                       <int> 324000, 307000, 256000, 248000, 490…\n#> $ original_loan_term                 <int> 360, 360, 360, 360, 360, 360, 360, …\n#> $ original_date                      <date> 2018-09-01, 2018-12-01, 2018-11-01…\n#> $ first_pay_date                     <date> 2018-11-01, 2019-02-01, 2019-01-01…\n#> $ original_ltv                       <dbl> 80, 90, 90, 90, 67, 69, 95, 80, 97,…\n#> $ original_cltv                      <dbl> 80, 90, 90, 90, 67, 69, 95, 80, 97,…\n#> $ number_of_borrowers                <dbl> 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1,…\n#> $ original_dti                       <dbl> 49, 44, 41, 40, 35, 31, 45, 47, 37,…\n#> $ original_borrower_credit_score     <dbl> 692, 722, 728, 730, 727, 798, 710, …\n#> $ first_time_home_buyer              <fct> N, N, N, Y, Y, N, N, N, Y, Y, N, N,…\n#> $ loan_purpose                       <fct> C, P, P, P, P, P, P, P, P, P, P, P,…\n#> $ property_type                      <fct> PU, PU, SF, SF, CO, PU, PU, CO, SF,…\n#> $ number_of_units                    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n#> $ occupancy_status                   <fct> P, P, S, P, P, P, P, P, P, P, P, P,…\n#> $ property_state                     <fct> CA, TX, NC, IL, CA, FL, TX, TX, IN,…\n#> $ zip                                <int> 925, 770, 286, 600, 945, 337, 773, …\n#> $ primary_mortgage_insurance_percent <dbl> NA, 25, 25, 25, NA, NA, 30, NA, 35,…\n#> $ product_type                       <fct> FRM, FRM, FRM, FRM, FRM, FRM, FRM, …\n#> $ original_coborrower_credit_score   <dbl> 665, NA, 738, 791, NA, 810, 778, NA…\n#> $ mortgage_insurance_type            <dbl> NA, 1, 1, 1, NA, NA, 1, NA, 1, 1, 1…\n#> $ relocation_mortgage_indicator      <fct> N, N, N, N, N, N, Y, N, N, N, N, N,…\n\n# 3.2 Performance Data ----\nsetDT(performance_data)\n\n#> Warning: One or more parsing issues, call `problems()` on your data frame for details,\n#> e.g.:\n#>   dat <- vroom(...)\n#>   problems(dat)\n\nperformance_data %>% glimpse()\n\n#> Rows: 3,105,039\n#> Columns: 31\n#> $ loan_id                                <fct> 100000913397, 100000913397, 100…\n#> $ monthly_reporting_period               <date> 2019-01-01, 2019-02-01, 2019-0…\n#> $ servicer_name                          <fct> \"OTHER\", NA, NA, NA, NA, NA, NA…\n#> $ current_interest_rate                  <dbl> 5.875, 5.875, 5.875, 5.875, 5.8…\n#> $ current_upb                            <dbl> NA, NA, NA, NA, NA, NA, 320968.…\n#> $ loan_age                               <dbl> 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…\n#> $ remaining_months_to_legal_maturity     <dbl> 357, 356, 355, 354, 353, 352, 3…\n#> $ adj_remaining_months_to_maturity       <dbl> 357, 356, 355, 354, 353, 352, 3…\n#> $ maturity_date                          <date> 2048-10-01, 2048-10-01, 2048-1…\n#> $ msa                                    <dbl> 40140, 40140, 40140, 40140, 401…\n#> $ current_loan_delinquency_status        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ modification_flag                      <fct> N, N, N, N, N, N, N, N, N, N, N…\n#> $ zero_balance_code                      <fct> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ zero_balance_effective_date            <date> NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ last_paid_installment_date             <date> NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ foreclosed_after                       <date> NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ disposition_date                       <date> NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ foreclosure_costs                      <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ prop_preservation_and_repair_costs     <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ asset_recovery_costs                   <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ misc_holding_expenses                  <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ holding_taxes                          <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ net_sale_proceeds                      <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ credit_enhancement_proceeds            <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ repurchase_make_whole_proceeds         <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ other_foreclosure_proceeds             <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ non_interest_bearing_upb               <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ principal_forgiveness_upb              <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ repurchase_make_whole_proceeds_flag    <fct> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ foreclosure_principal_write_off_amount <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ servicing_activity_indicator           <fct> N, N, N, N, N, N, N, N, N, N, N…\n\n# 4.0 DATA WRANGLING ----\n\n# 4.1 Joining / Merging Data ----\n\ncombined_data <- merge(x = acquisition_data, y = performance_data, \n                       by    = \"loan_id\", \n                       all.x = TRUE, \n                       all.y = FALSE)\n\ncombined_data %>% glimpse()\n\n#> Rows: 3,105,040\n#> Columns: 55\n#> $ loan_id                                <fct> 100000913397, 100000913397, 100…\n#> $ original_channel                       <fct> C, C, C, C, C, C, C, C, C, C, C…\n#> $ seller_name                            <fct> \"JPMORGAN CHASE BANK, NATIONAL …\n#> $ original_interest_rate                 <dbl> 5.875, 5.875, 5.875, 5.875, 5.8…\n#> $ original_upb                           <int> 324000, 324000, 324000, 324000,…\n#> $ original_loan_term                     <int> 360, 360, 360, 360, 360, 360, 3…\n#> $ original_date                          <date> 2018-09-01, 2018-09-01, 2018-0…\n#> $ first_pay_date                         <date> 2018-11-01, 2018-11-01, 2018-1…\n#> $ original_ltv                           <dbl> 80, 80, 80, 80, 80, 80, 80, 80,…\n#> $ original_cltv                          <dbl> 80, 80, 80, 80, 80, 80, 80, 80,…\n#> $ number_of_borrowers                    <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n#> $ original_dti                           <dbl> 49, 49, 49, 49, 49, 49, 49, 49,…\n#> $ original_borrower_credit_score         <dbl> 692, 692, 692, 692, 692, 692, 6…\n#> $ first_time_home_buyer                  <fct> N, N, N, N, N, N, N, N, N, N, N…\n#> $ loan_purpose                           <fct> C, C, C, C, C, C, C, C, C, C, C…\n#> $ property_type                          <fct> PU, PU, PU, PU, PU, PU, PU, PU,…\n#> $ number_of_units                        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ occupancy_status                       <fct> P, P, P, P, P, P, P, P, P, P, P…\n#> $ property_state                         <fct> CA, CA, CA, CA, CA, CA, CA, CA,…\n#> $ zip                                    <int> 925, 925, 925, 925, 925, 925, 9…\n#> $ primary_mortgage_insurance_percent     <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ product_type                           <fct> FRM, FRM, FRM, FRM, FRM, FRM, F…\n#> $ original_coborrower_credit_score       <dbl> 665, 665, 665, 665, 665, 665, 6…\n#> $ mortgage_insurance_type                <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ relocation_mortgage_indicator          <fct> N, N, N, N, N, N, N, N, N, N, N…\n#> $ monthly_reporting_period               <date> 2019-01-01, 2019-02-01, 2019-0…\n#> $ servicer_name                          <fct> \"OTHER\", NA, NA, NA, NA, NA, NA…\n#> $ current_interest_rate                  <dbl> 5.875, 5.875, 5.875, 5.875, 5.8…\n#> $ current_upb                            <dbl> NA, NA, NA, NA, NA, NA, 320968.…\n#> $ loan_age                               <dbl> 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…\n#> $ remaining_months_to_legal_maturity     <dbl> 357, 356, 355, 354, 353, 352, 3…\n#> $ adj_remaining_months_to_maturity       <dbl> 357, 356, 355, 354, 353, 352, 3…\n#> $ maturity_date                          <date> 2048-10-01, 2048-10-01, 2048-1…\n#> $ msa                                    <dbl> 40140, 40140, 40140, 40140, 401…\n#> $ current_loan_delinquency_status        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ modification_flag                      <fct> N, N, N, N, N, N, N, N, N, N, N…\n#> $ zero_balance_code                      <fct> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ zero_balance_effective_date            <date> NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ last_paid_installment_date             <date> NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ foreclosed_after                       <date> NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ disposition_date                       <date> NA, NA, NA, NA, NA, NA, NA, NA…\n#> $ foreclosure_costs                      <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ prop_preservation_and_repair_costs     <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ asset_recovery_costs                   <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ misc_holding_expenses                  <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ holding_taxes                          <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ net_sale_proceeds                      <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ credit_enhancement_proceeds            <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ repurchase_make_whole_proceeds         <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ other_foreclosure_proceeds             <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ non_interest_bearing_upb               <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ principal_forgiveness_upb              <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ repurchase_make_whole_proceeds_flag    <fct> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ foreclosure_principal_write_off_amount <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ servicing_activity_indicator           <fct> N, N, N, N, N, N, N, N, N, N, N…\n\n# Same operation with dplyr\n# performance_data %>%\n#   left_join(acquisition_data, by = \"loan_id\")\n\n# Preparing the Data Table\n\nsetkey(combined_data, \"loan_id\")\nkey(combined_data)\n\n#> [1] \"loan_id\"\n\n?setorder()\nsetorderv(combined_data, c(\"loan_id\", \"monthly_reporting_period\"))\n\n# 4.3 Select Columns ----\ncombined_data %>% dim()\n\n#> [1] 3105040      55\n\nkeep_cols <- c(\"loan_id\",\n               \"monthly_reporting_period\",\n               \"seller_name\",\n               \"current_interest_rate\",\n               \"current_upb\",\n               \"loan_age\",\n               \"remaining_months_to_legal_maturity\",\n               \"adj_remaining_months_to_maturity\",\n               \"current_loan_delinquency_status\",\n               \"modification_flag\",\n               \"zero_balance_code\",\n               \"foreclosure_costs\",\n               \"prop_preservation_and_repair_costs\",\n               \"asset_recovery_costs\",\n               \"misc_holding_expenses\",\n               \"holding_taxes\",\n               \"net_sale_proceeds\",\n               \"credit_enhancement_proceeds\",\n               \"repurchase_make_whole_proceeds\",\n               \"other_foreclosure_proceeds\",\n               \"non_interest_bearing_upb\",\n               \"principal_forgiveness_upb\",\n               \"repurchase_make_whole_proceeds_flag\",\n               \"foreclosure_principal_write_off_amount\",\n               \"servicing_activity_indicator\",\n               \"original_channel\",\n               \"original_interest_rate\",\n               \"original_upb\",\n               \"original_loan_term\",\n               \"original_ltv\",\n               \"original_cltv\",\n               \"number_of_borrowers\",\n               \"original_dti\",\n               \"original_borrower_credit_score\",\n               \"first_time_home_buyer\",\n               \"loan_purpose\",\n               \"property_type\",\n               \"number_of_units\",\n               \"property_state\",\n               \"occupancy_status\",\n               \"primary_mortgage_insurance_percent\",\n               \"product_type\",\n               \"original_coborrower_credit_score\",\n               \"mortgage_insurance_type\",\n               \"relocation_mortgage_indicator\")\n\ncombined_data <- combined_data[, ..keep_cols]\n\ncombined_data %>% dim()\n\n#> [1] 3105040      45\n\ncombined_data %>% glimpse()\n\n#> Rows: 3,105,040\n#> Columns: 45\n#> $ loan_id                                <fct> 100000913397, 100000913397, 100…\n#> $ monthly_reporting_period               <date> 2019-01-01, 2019-02-01, 2019-0…\n#> $ seller_name                            <fct> \"JPMORGAN CHASE BANK, NATIONAL …\n#> $ current_interest_rate                  <dbl> 5.875, 5.875, 5.875, 5.875, 5.8…\n#> $ current_upb                            <dbl> NA, NA, NA, NA, NA, NA, 320968.…\n#> $ loan_age                               <dbl> 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…\n#> $ remaining_months_to_legal_maturity     <dbl> 357, 356, 355, 354, 353, 352, 3…\n#> $ adj_remaining_months_to_maturity       <dbl> 357, 356, 355, 354, 353, 352, 3…\n#> $ current_loan_delinquency_status        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ modification_flag                      <fct> N, N, N, N, N, N, N, N, N, N, N…\n#> $ zero_balance_code                      <fct> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ foreclosure_costs                      <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ prop_preservation_and_repair_costs     <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ asset_recovery_costs                   <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ misc_holding_expenses                  <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ holding_taxes                          <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ net_sale_proceeds                      <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ credit_enhancement_proceeds            <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ repurchase_make_whole_proceeds         <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ other_foreclosure_proceeds             <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ non_interest_bearing_upb               <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ principal_forgiveness_upb              <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ repurchase_make_whole_proceeds_flag    <fct> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ foreclosure_principal_write_off_amount <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ servicing_activity_indicator           <fct> N, N, N, N, N, N, N, N, N, N, N…\n#> $ original_channel                       <fct> C, C, C, C, C, C, C, C, C, C, C…\n#> $ original_interest_rate                 <dbl> 5.875, 5.875, 5.875, 5.875, 5.8…\n#> $ original_upb                           <int> 324000, 324000, 324000, 324000,…\n#> $ original_loan_term                     <int> 360, 360, 360, 360, 360, 360, 3…\n#> $ original_ltv                           <dbl> 80, 80, 80, 80, 80, 80, 80, 80,…\n#> $ original_cltv                          <dbl> 80, 80, 80, 80, 80, 80, 80, 80,…\n#> $ number_of_borrowers                    <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n#> $ original_dti                           <dbl> 49, 49, 49, 49, 49, 49, 49, 49,…\n#> $ original_borrower_credit_score         <dbl> 692, 692, 692, 692, 692, 692, 6…\n#> $ first_time_home_buyer                  <fct> N, N, N, N, N, N, N, N, N, N, N…\n#> $ loan_purpose                           <fct> C, C, C, C, C, C, C, C, C, C, C…\n#> $ property_type                          <fct> PU, PU, PU, PU, PU, PU, PU, PU,…\n#> $ number_of_units                        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ property_state                         <fct> CA, CA, CA, CA, CA, CA, CA, CA,…\n#> $ occupancy_status                       <fct> P, P, P, P, P, P, P, P, P, P, P…\n#> $ primary_mortgage_insurance_percent     <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ product_type                           <fct> FRM, FRM, FRM, FRM, FRM, FRM, F…\n#> $ original_coborrower_credit_score       <dbl> 665, 665, 665, 665, 665, 665, 6…\n#> $ mortgage_insurance_type                <dbl> NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ relocation_mortgage_indicator          <fct> N, N, N, N, N, N, N, N, N, N, N…\n\ncombined_data$current_loan_delinquency_status %>% unique()\n\n#>  [1]  0 NA  1  2  3  4  5  6  7  8  9 10 11 12\n\n##  0 NA  1  2  3  4  5  6  7  8  9 10 11 12\n\n# # or:\n# combined_data[,current_loan_delinquency_status] %>% unique()\n# ##  0 NA  1  2  3  4  5  6  7  8  9 10 11 12\n\n# 4.4 Grouped Mutations ----\n# - Add response variable (Predict wether loan will become delinquent in next 3 months)\n\n# dplyr\ntemp <- combined_data %>%\n  group_by(loan_id) %>%\n  mutate(gt_1mo_behind_in_3mo_dplyr = lead(current_loan_delinquency_status, n = 3) >= 1) %>%\n  ungroup()\n\ncombined_data %>% dim()\n\n#> [1] 3105040      45\n\ntemp %>% dim()\n\n#> [1] 3105040      46\n\n# data.table\ncombined_data[, gt_1mo_behind_in_3mo := lead(current_loan_delinquency_status, n = 3) >= 1,\n              by = loan_id]\n\ncombined_data %>% dim()\n\n#> [1] 3105040      46\n\n# Remove the temp variable\nrm(temp)\n\n\n# 5.1 How many loans in a month ----\ncombined_data[!is.na(monthly_reporting_period), .N, by = monthly_reporting_period]\n\n\n\n  \n\n\ncombined_data %>%\n    filter(!is.na(monthly_reporting_period)) %>%\n    count(monthly_reporting_period) \n\n\n\n  \n\n\n# 5.2 Which loans have the most outstanding delinquencies ----\n# data.table\ncombined_data[current_loan_delinquency_status >= 1, \n              list(loan_id, monthly_reporting_period, current_loan_delinquency_status, seller_name, current_upb)][\n                , max(current_loan_delinquency_status), by = loan_id][\n                  order(V1, decreasing = TRUE)]\n\n\n\n  \n\n\n# dplyr\n# combined_data %>%\n#   group_by(loan_id) %>%\n#   summarise(total_delinq = max(current_loan_delinquency_status)) %>%\n#   ungroup() %>%\n#   arrange(desc(total_delinq))\n\n# 5.3 Get last unpaid balance value for delinquent loans ----\n# data.table\ncombined_data[current_loan_delinquency_status >= 1, .SD[.N], by = loan_id][\n  !is.na(current_upb)][\n  order(-current_upb), .(loan_id, monthly_reporting_period, current_loan_delinquency_status, seller_name, current_upb)  \n  ]\n\n\n\n  \n\n\n# dplyr\n# combined_data %>%\n#   filter(current_loan_delinquency_status >= 1) %>%\n#   filter(!is.na(current_upb)) %>%\n#\n#   group_by(loan_id) %>%\n#   slice(n()) %>%\n#   ungroup() %>%\n#\n#   arrange(desc(current_upb)) %>%\n#   select(loan_id, monthly_reporting_period, current_loan_delinquency_status, seller_name, current_upb)\n\n\n# 5.4 Loan Companies with highest unpaid balance\n# data.table\nupb_by_company_dt <- combined_data[!is.na(current_upb), .SD[.N], by = loan_id][\n  , .(sum_current_upb = sum(current_upb, na.rm = TRUE), cnt_current_upb = .N), by = seller_name][\n    order(sum_current_upb, decreasing = TRUE)]\n\nupb_by_company_dt\n\n\n\n  \n\n\n# dplyr\n# upb_by_company_tbl <- combined_data %>%\n#\n#   filter(!is.na(current_upb)) %>%\n#   group_by(loan_id) %>%\n#   slice(n()) %>%\n#   ungroup() %>%\n#\n#   group_by(seller_name) %>%\n#   summarise(\n#     sum_current_upb = sum(current_upb, na.rm = TRUE),\n#     cnt_current_upb = n()\n#   ) %>%\n#   ungroup() %>%\n#\n#   arrange(desc(sum_current_upb))\n\n\n2 Challenge 3.1\n\n# Challenge 3 ----\n\n# 1.0 LIBRARIES ----\n\n# Tidyverse\nlibrary(tidyverse)\nlibrary(vroom)\n\n# Data Table\nlibrary(data.table)\n\n# 2.0 DATA ----\n\n# Import assignee table\ncol_types <- list(\n  id = col_character(),\n  type = col_character(),\n  organization = col_character()\n)\n\nassignee_tbl <- vroom(\n            file       = \"./../../patents/assignee.tsv\", \n            delim      = \"\\t\", \n            col_types  = col_types,\n            na         = c(\"\", \"NA\", \"NULL\")\n        )\n\n# import patent table\ncol_types <- list(\n  id = col_character(),\n  date = col_date(\"%Y-%m-%d\"),\n  num_claims = col_double()\n)\n\npatent_tbl <- vroom(\n            file       = \"./../../patents/patent.tsv\", \n            delim      = \"\\t\", \n            col_types  = col_types,\n            na         = c(\"\", \"NA\", \"NULL\")\n        )\n\n# Import patent assignee table\ncol_types <- list(\n  patent_id = col_character(),\n  assignee_id = col_character()\n)\n\npatent_asignee_tbl <- vroom(\n            file       = \"./../../patents/patent_assignee.tsv\", \n            delim      = \"\\t\", \n            col_types  = col_types,\n            na         = c(\"\", \"NA\", \"NULL\")\n        )\n\n# Import uspc table \ncol_types <- list(\n  patent_id = col_character(),\n  mainclass_id = col_character(),\n  sequence = col_character()\n)\n\nuspc_tbl <- vroom(\n            file       = \"./../../patents/uspc.tsv\", \n            delim      = \"\\t\", \n            col_types  = col_types,\n            na         = c(\"\", \"NA\", \"NULL\")\n        )\n\n# Convert to data.table\nclass(assignee_tbl)\n\n#> [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\"\n\nsetDT(assignee_tbl)\nclass(assignee_tbl)\n\n#> [1] \"data.table\" \"data.frame\"\n\nclass(patent_tbl)\n\n#> [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\"\n\nsetDT(patent_tbl)\nclass(patent_tbl)\n\n#> [1] \"data.table\" \"data.frame\"\n\nclass(patent_asignee_tbl)\n\n#> [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\"\n\nsetDT(patent_asignee_tbl)\nclass(patent_asignee_tbl)\n\n#> [1] \"data.table\" \"data.frame\"\n\nclass(uspc_tbl)\n\n#> [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\"\n\nsetDT(uspc_tbl)\nclass(uspc_tbl)\n\n#> [1] \"data.table\" \"data.frame\"\n\n# 3.0 ANALYSIS ----\n# rename id in assignee table to assignee_id\nassignee_tbl <- assignee_tbl %>% rename(assignee_id = id)\n \n# rename id in patent table to patent_id\npatent_tbl <- patent_tbl %>% rename(patent_id = id)\n\n# Write organization names to the respective id\ncombi_data <- merge(x = patent_asignee_tbl, y = assignee_tbl, \n                       by    = \"assignee_id\", \n                       all.x = TRUE, \n                       all.y = FALSE)\n\n# Companies with the most patents:\npatent_by_company_tbl <-combi_data[!is.na(assignee_id), .(sum_patents = .N), by = assignee_id][\n  order(sum_patents, decreasing = TRUE)]\n\n# re add the company names \npatent_by_company_tbl <- merge(x = patent_by_company_tbl, y = assignee_tbl, \n                       by    = \"assignee_id\", \n                       all.x = TRUE, \n                       all.y = FALSE)\n\n# rearrange\npatent_by_company_tbl <- patent_by_company_tbl %>% select(organization, sum_patents, assignee_id)\n\n# order\npatent_by_company_tbl <- patent_by_company_tbl[order(sum_patents, decreasing = TRUE)]\n\n# top 10 \nhead(patent_by_company_tbl, 10)\n\n\n\n  \n\n\n\n\n3 Challenge 3.2\n\n# Add date to patent_asignee_tbl \ncombi_data1 <- merge(x = combi_data, y = patent_tbl, \n                       by    = \"patent_id\", \n                       all.x = TRUE, \n                       all.y = FALSE)\n\n# Companies with the most patents in April \npatent_by_company_april_tbl <-combi_data1[month(date) == 4 ][!is.na(assignee_id), .(sum_patents = .N), by = assignee_id]\n\n# re add the company names \npatent_by_company_april_tbl <- merge(x = patent_by_company_april_tbl, y = assignee_tbl, \n                       by    = \"assignee_id\", \n                       all.x = TRUE, \n                       all.y = FALSE)\n\n# rearrange\npatent_by_company_april_tbl <- patent_by_company_april_tbl %>% select(organization, sum_patents, assignee_id)\n\n# order\npatent_by_company_april_tbl <- patent_by_company_april_tbl[order(sum_patents, decreasing = TRUE)]\n\n# top 10 \nhead(patent_by_company_april_tbl, 10)\n\n\n\n  \n\n\n\n\n4 Challenge 3.3\n\n# Top 10 companies with the most patents\ntop_10_tbl <- head(patent_by_company_tbl, 10)\n\n# Get patents of top 10 companies \ntop_10_patents_tbl <- merge(x = top_10_tbl, y = combi_data, \n                       by    = \"assignee_id\", \n                       all.x = TRUE, \n                       all.y = FALSE)\n\n# rearrange\ntop_10_patents_tbl <- top_10_patents_tbl %>% select(patent_id, assignee_id)\n\n# Add the uspc class to each patent_id \ntop_10_patents_tbl <- merge(x = top_10_patents_tbl, y = uspc_tbl, \n                       by    = \"patent_id\", \n                       all.x = TRUE, \n                       all.y = FALSE)\n\n# most common mainclass_id\ntop_mainclass_tbl <- top_10_patents_tbl[!is.na(mainclass_id), .(occurances = .N), by = mainclass_id][order(occurances, decreasing = TRUE)]\n\n# top 5 main classes:\nhead(top_mainclass_tbl, 5)"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Tidyverse",
    "section": "",
    "text": "(Code mainly from startupengineer templates)\nBusiness code commented out because of website changes.\n\n# WEBSCRAPING ----\n\n# 1.0 LIBRARIES ----\n\nlibrary(tidyverse) # Main Package - Loads dplyr, purrr, etc.\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(rvest)     # HTML Hacking & Web Scraping\n\n#> \n#> Attaching package: 'rvest'\n#> \n#> The following object is masked from 'package:readr':\n#> \n#>     guess_encoding\n\nlibrary(xopen)     # Quickly opening URLs\nlibrary(jsonlite)  # converts JSON files to R objects\n\n#> \n#> Attaching package: 'jsonlite'\n#> \n#> The following object is masked from 'package:purrr':\n#> \n#>     flatten\n\nlibrary(glue)      # concatenate strings\nlibrary(stringi)   # character string/text processing\nlibrary(RSQLite)\nlibrary(httr)\n\n# Import and list tables\ncon <- RSQLite::dbConnect(drv = SQLite(), dbname = \"./../../00_data/02_chinook/Chinook_Sqlite.sqlite\")\ndbListTables(con)\n\n#>  [1] \"Album\"         \"Artist\"        \"Customer\"      \"Employee\"     \n#>  [5] \"Genre\"         \"Invoice\"       \"InvoiceLine\"   \"MediaType\"    \n#>  [9] \"Playlist\"      \"PlaylistTrack\" \"Track\"\n\n# Look at a table\ntbl(con, \"Album\")\n\n\n\n  \n\n\n# Save data to memory\nalbum_tbl <- tbl(con, \"Album\") %>% collect()\n\n# Close database connection\ndbDisconnect(con)\ncon\n\n#> <SQLiteConnection>\n#>   DISCONNECTED\n\n# glue string interpolation example\nname <- \"Fred\"\nglue('My name is {name}.')\n\n#> My name is Fred.\n\n# API GET example\nresp <- GET(\"https://swapi.dev/api/people/1/\")\n\n# Wrapped into a function\nsw_api <- function(path) {\n  url <- modify_url(url = \"https://swapi.dev\", path = glue(\"/api{path}\"))\n  resp <- GET(url)\n  stop_for_status(resp) # automatically throws an error if a request did not succeed\n}\n\n# API GET function example\nresp <- sw_api(\"/people/1\")\nresp\n\n#> Response [https://swapi.dev/api/people/1]\n#>   Date: 2023-05-22 21:13\n#>   Status: 200\n#>   Content-Type: application/json\n#>   Size: 647 B\n\n# convert API response body\nresp_extracted <- rawToChar(resp$content)  %>% fromJSON()\nresp_extracted\n\n#> $name\n#> [1] \"Luke Skywalker\"\n#> \n#> $height\n#> [1] \"172\"\n#> \n#> $mass\n#> [1] \"77\"\n#> \n#> $hair_color\n#> [1] \"blond\"\n#> \n#> $skin_color\n#> [1] \"fair\"\n#> \n#> $eye_color\n#> [1] \"blue\"\n#> \n#> $birth_year\n#> [1] \"19BBY\"\n#> \n#> $gender\n#> [1] \"male\"\n#> \n#> $homeworld\n#> [1] \"https://swapi.dev/api/planets/1/\"\n#> \n#> $films\n#> [1] \"https://swapi.dev/api/films/1/\" \"https://swapi.dev/api/films/2/\"\n#> [3] \"https://swapi.dev/api/films/3/\" \"https://swapi.dev/api/films/6/\"\n#> \n#> $species\n#> list()\n#> \n#> $vehicles\n#> [1] \"https://swapi.dev/api/vehicles/14/\" \"https://swapi.dev/api/vehicles/30/\"\n#> \n#> $starships\n#> [1] \"https://swapi.dev/api/starships/12/\" \"https://swapi.dev/api/starships/22/\"\n#> \n#> $created\n#> [1] \"2014-12-09T13:50:51.644000Z\"\n#> \n#> $edited\n#> [1] \"2014-12-20T21:17:56.891000Z\"\n#> \n#> $url\n#> [1] \"https://swapi.dev/api/people/1/\"\n\n# Test list\ndata_list <- list(strings= c(\"string1\", \"string2\"), \n                  numbers = c(1,2,3), \n                  TRUE, \n                  100.23, \n                  tibble(\n                    A = c(1,2), \n                    B = c(\"x\", \"y\")\n                  )\n)\n\n# access list\nresp %>% \n  .$content %>% \n  rawToChar() %>% \n  fromJSON()\n\n#> $name\n#> [1] \"Luke Skywalker\"\n#> \n#> $height\n#> [1] \"172\"\n#> \n#> $mass\n#> [1] \"77\"\n#> \n#> $hair_color\n#> [1] \"blond\"\n#> \n#> $skin_color\n#> [1] \"fair\"\n#> \n#> $eye_color\n#> [1] \"blue\"\n#> \n#> $birth_year\n#> [1] \"19BBY\"\n#> \n#> $gender\n#> [1] \"male\"\n#> \n#> $homeworld\n#> [1] \"https://swapi.dev/api/planets/1/\"\n#> \n#> $films\n#> [1] \"https://swapi.dev/api/films/1/\" \"https://swapi.dev/api/films/2/\"\n#> [3] \"https://swapi.dev/api/films/3/\" \"https://swapi.dev/api/films/6/\"\n#> \n#> $species\n#> list()\n#> \n#> $vehicles\n#> [1] \"https://swapi.dev/api/vehicles/14/\" \"https://swapi.dev/api/vehicles/30/\"\n#> \n#> $starships\n#> [1] \"https://swapi.dev/api/starships/12/\" \"https://swapi.dev/api/starships/22/\"\n#> \n#> $created\n#> [1] \"2014-12-09T13:50:51.644000Z\"\n#> \n#> $edited\n#> [1] \"2014-12-20T21:17:56.891000Z\"\n#> \n#> $url\n#> [1] \"https://swapi.dev/api/people/1/\"\n\n# directly parse body\n# content(resp, as = \"text\")\n# content(resp, as = \"parsed\")\ncontent(resp)\n\n#> $name\n#> [1] \"Luke Skywalker\"\n#> \n#> $height\n#> [1] \"172\"\n#> \n#> $mass\n#> [1] \"77\"\n#> \n#> $hair_color\n#> [1] \"blond\"\n#> \n#> $skin_color\n#> [1] \"fair\"\n#> \n#> $eye_color\n#> [1] \"blue\"\n#> \n#> $birth_year\n#> [1] \"19BBY\"\n#> \n#> $gender\n#> [1] \"male\"\n#> \n#> $homeworld\n#> [1] \"https://swapi.dev/api/planets/1/\"\n#> \n#> $films\n#> $films[[1]]\n#> [1] \"https://swapi.dev/api/films/1/\"\n#> \n#> $films[[2]]\n#> [1] \"https://swapi.dev/api/films/2/\"\n#> \n#> $films[[3]]\n#> [1] \"https://swapi.dev/api/films/3/\"\n#> \n#> $films[[4]]\n#> [1] \"https://swapi.dev/api/films/6/\"\n#> \n#> \n#> $species\n#> list()\n#> \n#> $vehicles\n#> $vehicles[[1]]\n#> [1] \"https://swapi.dev/api/vehicles/14/\"\n#> \n#> $vehicles[[2]]\n#> [1] \"https://swapi.dev/api/vehicles/30/\"\n#> \n#> \n#> $starships\n#> $starships[[1]]\n#> [1] \"https://swapi.dev/api/starships/12/\"\n#> \n#> $starships[[2]]\n#> [1] \"https://swapi.dev/api/starships/22/\"\n#> \n#> \n#> $created\n#> [1] \"2014-12-09T13:50:51.644000Z\"\n#> \n#> $edited\n#> [1] \"2014-12-20T21:17:56.891000Z\"\n#> \n#> $url\n#> [1] \"https://swapi.dev/api/people/1/\"\n\n# call to alpha vantage api\nresp <- GET('https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol=WDI.DE')\nresp\n\n#> Response [https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol=WDI.DE]\n#>   Date: 2023-05-22 21:13\n#>   Status: 200\n#>   Content-Type: application/json\n#>   Size: 189 B\n#> {\n#>     \"Error Message\": \"the parameter apikey is invalid or missing. Please clai...\n\n# test the renviron file\nuserid <- Sys.getenv('userid')\npwd <- Sys.getenv('pwd')\nkey <- Sys.getenv('key')\n\n# web scraping example\n# get the URL for the wikipedia page with all S&P 500 symbols\nurl <- \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n# use that URL to scrape the S&P 500 table using rvest\nlibrary(rvest)\nsp_500 <- url %>%\n  # read the HTML from the webpage\n  read_html() %>%\n  # Get the nodes with the id\n  html_nodes(css = \"#constituents\") %>%\n  # html_nodes(xpath = \"//*[@id='constituents']\"\") %>% \n  # Extract the table and turn the list into a tibble\n  html_table() %>% \n  .[[1]] %>% \n  as_tibble()\n\n# web scraping example 2\nurl  <- \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\"\nhtml <- url %>% \n  read_html\n\n# get the ranks\nrank <-  html %>% \n  html_nodes(css = \".titleColumn\") %>% \n  html_text() %>% \n  # Extrag all digits between \" \" and \".\\n\" The \"\\\" have to be escaped\n  # You can use Look ahead \"<=\" and Look behind \"?=\" for this\n  stringr::str_extract(\"(?<= )[0-9]*(?=\\\\.\\\\n)\")%>% \n  # Make all values numeric\n  as.numeric()\n\n# get the titles\ntitle <- html %>% \n  html_nodes(\".titleColumn > a\") %>% \n  html_text()\n\n# get the year\nyear <- html %>% \n  html_nodes(\".titleColumn .secondaryInfo\") %>%\n  html_text() %>% \n  # Extract numbers\n  stringr::str_extract(pattern = \"[0-9]+\") %>% \n  as.numeric()\n\n# get the people\npeople <- html %>% \n  html_nodes(\".titleColumn > a\") %>% \n  html_attr(\"title\")\n\n# get the ratings\nrating <- html %>% \n  html_nodes(css = \".imdbRating > strong\") %>% \n  html_text() %>% \n  as.numeric()\n\n# get the number of ratings\nnum_ratings <- html %>% \n  html_nodes(css = \".imdbRating > strong\") %>% \n  html_attr('title') %>% \n  # Extract the numbers and remove the comma to make it numeric values\n  stringr::str_extract(\"(?<=based on ).*(?=\\ user ratings)\" ) %>% \n  stringr::str_replace_all(pattern = \",\", replacement = \"\") %>% \n  as.numeric()\n\n# merge everything\nimdb_tbl <- tibble(rank, title, year, people, rating, num_ratings)\n\n# # read bike data from json file\n# bike_data_lst <- fromJSON(\"bike_data.json\")\n# # Open the data by clicking on it in the environment or by running View()\n# View(bike_data_lst)\n# \n# # color path\n# # productDetail --> variationAttributes --> values --> [[1]] --> displayValue\n# bike_data_lst[[\"productDetail\"]][[\"variationAttributes\"]][[\"values\"]][[1]][[\"displayValue\"]]\n# \n# # color extraction using pluck\n# bike_data_lst %>%\n#   purrr::pluck(\"productDetail\", \"variationAttributes\", \"values\", 1, \"displayValue\")\n\n# Scraping example\n  # # 1.1 collect product families ----\n  # \n  # url_home          <- \"https://www.canyon.com/en-de\"\n  # xopen(url_home) # Open links directly from RStudio to inspect them\n  # \n  # # Read in the HTML for the entire webpage\n  # html_home         <- read_html(url_home)\n  # \n  # # Web scrape the ids for the families\n  # bike_family_tbl <- html_home %>%\n  #   \n  #   # Get the nodes for the families ...\n  #   html_nodes(css = \".js-navigationDrawer__list--secondary\") %>%\n  #   # ...and extract the information of the id attribute\n  #   html_attr('id') %>%\n  #   \n  #   # Remove the product families Gear and Outlet and Woman \n  #   # (because the female bikes are also listed with the others)\n  #   discard(.p = ~stringr::str_detect(.x,\"WMN|WOMEN|GEAR|OUTLET\")) %>%\n  #   \n  #   # Convert vector to tibble\n  #   enframe(name = \"position\", value = \"family_class\") %>%\n  #   \n  #   # Add a hashtag so we can get nodes of the categories by id (#)\n  #   mutate(\n  #     family_id = str_glue(\"#{family_class}\")\n  #   )\n  # \n  # bike_family_tbl\n  # \n  # # 1.2 COLLECT PRODUCT CATEGORIES ----\n  # \n  # # Combine all Ids to one string so that we will get all nodes at once\n  # # (seperated by the OR operator \",\")\n  # family_id_css <- bike_family_tbl %>%\n  #   pull(family_id) %>%\n  #   stringr::str_c(collapse = \", \")\n  # family_id_css\n  # ## \"#js-navigationList-ROAD, #js-navigationList-MOUNTAIN, #js-navigationList-EBIKES, #js-navigationList-HYBRID-CITY, #js-navigationList-YOUNGHEROES\"\n  # \n  # # Extract the urls from the href attribute\n  # bike_category_tbl <- html_home %>%\n  #   \n  #   # Select nodes by the ids\n  #   html_nodes(css = family_id_css) %>%\n  #   \n  #   # Going further down the tree and select nodes by class\n  #   # Selecting two classes makes it specific enough\n  #   html_nodes(css = \".navigationListSecondary__listItem .js-ridestyles\") %>%\n  #   html_attr('href') %>%\n  #   \n  #   # Convert vector to tibble\n  #   enframe(name = \"position\", value = \"subdirectory\") %>%\n  #   \n  #   # Add the domain, because we will get only the subdirectories\n  #   mutate(\n  #     url = glue(\"https://www.canyon.com{subdirectory}\")\n  #   ) %>%\n  #   \n  #   # Some categories are listed multiple times.\n  #   # We only need unique values\n  #   distinct(url)\n  # \n  # bike_category_tbl\n  # \n  # # 2.0 COLLECT BIKE DATA ----\n  # \n  # # 2.1 Get URL for each bike of the Product categories\n  # \n  # # select first bike category url\n  # bike_category_url <- bike_category_tbl$url[1]\n  # \n  # # Alternatives for selecting values\n  # # bike_category_url <- bike_category_tbl %$% url %>% .[1]\n  # # bike_category_url <- bike_category_tbl %>% pull(url) %>% .[1]\n  # # bike_category_url <- deframe(bike_category_tbl[1,])\n  # # bike_category_url <- bike_category_tbl %>% first %>% first\n  # \n  # xopen(bike_category_url)\n  # \n  # # Get the URLs for the bikes of the first category\n  # html_bike_category  <- read_html(bike_category_url)\n  # bike_url_tbl        <- html_bike_category %>%\n  #   \n  #   # Get the 'a' nodes, which are hierarchally underneath \n  #   # the class productTile__contentWrapper\n  #   html_nodes(css = \".productTile__contentWrapper > a\") %>%\n  #   html_attr(\"href\") %>%\n  #   \n  #   # Remove the query parameters of the URL (everything after the '?')\n  #   str_remove(pattern = \"\\\\?.*\") %>%\n  #   \n  #   # Convert vector to tibble\n  #   enframe(name = \"position\", value = \"url\")\n  # \n  # # 2.1.2 Extract the descriptions (since we have retrieved the data already)\n  # bike_desc_tbl <- html_bike_category %>%\n  #   \n  #   # Get the nodes in the meta tag where the attribute itemprop equals description\n  #   html_nodes('.productTile__productSummaryLeft > meta[itemprop=\"description\"]') %>%\n  #   \n  #   # Extract the content of the attribute content\n  #   html_attr(\"content\") %>%\n  #   \n  #   # Convert vector to tibble\n  #   enframe(name = \"position\", value = \"description\")\n  # \n  # # 2.1.3 Get even more data from JSON files\n  # bike_json_tbl  <- html_bike_category %>%\n  #   \n  #   html_nodes(css = '.productGrid__listItem.xlt-producttile > div') %>%\n  #   html_attr(\"data-gtm-impression\") %>%\n  #   \n  #   # Convert the JSON format to dataframe\n  #   # map runs that function on each element of the list\n  #   map(fromJSON) %>% # need JSON ### need lists\n  #   \n  #   # Extract relevant information of the nested list\n  #   map(purrr::pluck, 2, \"impressions\") %>% # Need purrr and expl above\n  #   \n  #   # Set \"not defined\" and emtpy fields to NA (will be easier to work with)\n  #   map(na_if, \"not defined\") %>%\n  #   map(na_if, \"\") %>%\n  #   \n  #   # The class of dimension56 and price varies between numeric and char.\n  #   # This converts this column in each list to numeric\n  #   # across allows to perform the same operation on multiple columns\n  #   map(~mutate(., across(c(\"dimension56\",\"price\"), as.numeric))) %>%\n  #   \n  #   # Stack all lists together\n  #   bind_rows() %>%\n  #   # Convert to tibble so that we have the same data format\n  #   as_tibble() %>%\n  #   \n  #   # Add consecutive numbers so that we can bind all data together\n  #   # You could have also just use bind_cols()\n  #   rowid_to_column(var='position') %>%\n  #   left_join(bike_desc_tbl) %>%\n  #   left_join(bike_url_tbl)\n  # \n  # # 2.2 Wrap it into a function ----\n  # get_bike_data <- function(url) {\n  #   \n  #   html_bike_category <- read_html(url)\n  #   \n  #   # Get the URLs\n  #   bike_url_tbl  <- html_bike_category %>%\n  #     html_nodes(css = \".productTile__contentWrapper > a\") %>%\n  #     html_attr(\"href\") %>%\n  #     str_remove(pattern = \"\\\\?.*\") %>%\n  #     enframe(name = \"position\", value = \"url\")\n  #   \n  #   # Get the descriptions\n  #   bike_desc_tbl <- html_bike_category %>%\n  #     html_nodes(css = '.productTile__productSummaryLeft > \n  #                       meta[itemprop=\"description\"]') %>%\n  #     html_attr(\"content\") %>%\n  #     enframe(name = \"position\", value = \"description\")\n  #   \n  #   # Get JSON data\n  #   bike_json_tbl <- html_bike_category %>%\n  #     html_nodes(css = '.productGrid__listItem.xlt-producttile > div') %>%\n  #     html_attr(\"data-gtm-impression\") %>%\n  #     map(fromJSON) %>% # need JSON ### need lists\n  #     map(purrr::pluck, 2, \"impressions\") %>% \n  #     map(na_if, \"not defined\") %>%\n  #     map(na_if, \"\") %>%\n  #     map(~mutate(., across(c(\"dimension56\",\"price\"), as.numeric))) %>%\n  #     bind_rows() %>%\n  #     as_tibble() %>%\n  #     rowid_to_column(var='position') %>%\n  #     left_join(bike_desc_tbl) %>%\n  #     left_join(bike_url_tbl)\n  # }\n  # \n  # # Run the function with the first url to check if it is working\n  # bike_category_url <- bike_category_tbl$url[1]\n  # bike_data_tbl     <- get_bike_data(url = bike_category_url)\n  # \n  # bike_data_tbl\n  # \n  # # 2.3.1a Map the function against all urls\n  # \n  # # Extract the urls as a character vector\n  # bike_category_url_vec <- bike_category_tbl %>% \n  #   pull(url)\n  # \n  # # Run the function with every url as an argument\n  # bike_data_lst <- map(bike_category_url_vec, get_bike_data)\n  # \n  # # Merge the list into a tibble\n  # bike_data_tbl <- bind_rows(bike_data_lst)\n  # saveRDS(bike_data_tbl, \"bike_data_tbl.rds\")\n  # \n  # # 2.3.1b Alternative with a for loop\n  # \n  # # Create an empty tibble, that we can populate\n  # bike_data_tbl <- tibble()\n  # \n  # # Loop through all urls\n  # for (i in seq_along(bike_category_tbl$url)) {\n  #   \n  #   bike_category_url <- bike_category_tbl$url[i]\n  #   bike_data_tbl     <- bind_rows(bike_data_tbl, get_bike_data(bike_category_url))\n  #   \n  #   # Wait between each request to reduce the load on the server \n  #   # Otherwise we could get blocked\n  #   Sys.sleep(5)\n  #   \n  #   # print the progress\n  #   print(i)\n  #   \n  # }\n  # \n  # # Check for duplicates\n  # bike_data_tbl %>%\n  #   group_by(id) %>%\n  #   filter(n()>1) %>%\n  #   arrange(id) %>% \n  #   View()\n  # \n  # # Filter non Canyon bikes (based on id length) and add an empty column for the colors\n  # bike_data_cleaned_tbl <- bike_data_tbl %>%\n  #   \n  #   # Filter for bikes. Only unique ones\n  #   filter(nchar(.$id) == 4) %>%\n  #   filter(!(name %>% str_detect(\"Frameset\"))) %>%\n  #   distinct(id, .keep_all = T) %>%\n  #   \n  #   # Split categories (Speedmax had to be treated individually)\n  #   mutate(category = replace(category, \n  #                             name == \"Speedmax CF SLX 8.0 SL\", \"Road/Triathlon Bike/Speedmax\")) %>%\n  #   separate(col = category, into = c(\"category_1\",\n  #                                     \"category_2\",\n  #                                     \"category_3\"),\n  #            sep = \"(?<!\\\\s)/(?!\\\\s)\") %>%\n  #   \n  #   # Renaming\n  #   rename(\"year\"       = \"dimension50\") %>%\n  #   rename(\"model\"      = \"name\") %>%\n  #   rename(\"gender\"     = \"dimension63\") %>%\n  #   rename(\"price_euro\" = \"metric4\") %>%\n  #   \n  #   # Fix years manually (have checked the website)\n  #   mutate(year = replace_na(year, 2021)) %>%\n  #   \n  #   # Add frame material\n  #   mutate(frame_material = case_when(\n  #     model %>% str_detect(\" CF \") ~ \"carbon\",\n  #     model %>% str_detect(\" CFR \") ~ \"carbon\",\n  #     TRUE ~ \"aluminium\"\n  #   )\n  #   ) %>%\n  #   \n  #   # Select and order columns\n  #   select(-c(position, brand, variant, starts_with(\"dim\"), \n  #             quantity, feedProductId, price, metric5)) %>%\n  #   select(id, model, year, frame_material, price_euro, everything())\n  # \n  # saveRDS(bike_data_cleaned_tbl, \"bike_data_cleaned_tbl.rds\")\n  # \n  # # 3.1a Get all color variations for each bike\n  # \n  # # Extract all bike urls\n  # bike_url_vec <- bike_data_cleaned_tbl %>% \n  #   pull(url)\n  # \n  # # Create function to get the variations\n  # get_colors <- function(url) {\n  #   \n  #   url %>%\n  #     \n  #     read_html() %>%\n  #     \n  #     # Get all 'script nodes' and convert to char\n  #     html_nodes(css = \"script\") %>%\n  #     as.character() %>%\n  #     \n  #     # Select the node, that contains 'window.deptsfra'\n  #     str_subset(pattern = \"window.deptsfra\") %>%\n  #     \n  #     # remove the chars that do not belong to the json\n  #     # 1. replace at the beginning everything until the first \"{\" with \"\"\n  #     str_replace(\"^[^\\\\{]+\", \"\") %>%\n  #     # 2. replace at the end everything after the last \"}\" with \"\"\n  #     str_replace(\"[^\\\\}]+$\", \"\") %>%\n  #     \n  #     # Convert from json to an r object and pick the relevant values\n  #     fromJSON() %>%\n  #     purrr::pluck(\"productDetail\", \"variationAttributes\", \"values\", 1, \"value\")\n  # }\n  # \n  # # Run the function over all urls and add result to bike_data_cleaned_tbl\n  # # This will take a long time (~ 20-30 minutes) because we have to iterate over many bikes\n  # bike_data_colors_tbl <- bike_data_cleaned_tbl %>% \n  #   mutate(colors = map(bike_url_vec, get_colors))\n  # \n  # saveRDS(bike_data_colors_tbl, \"bike_data_colors_tbl.rds\")\n  # \n  # library(furrr)     # Parallel Processing using purrr (iteration)\n  # plan(\"multiprocess\")\n  # bike_data_colors_tbl <- bike_data_cleaned_tbl %>% \n  #   mutate(colors = future_map(bike_url_vec, get_colors))\n  # \n  # # 3.2 Create the urls for each variation\n  # \n  # bike_data_colors_tbl <- bike_data_colors_tbl %>%\n  #   \n  #   # Create entry for each color variation\n  #   unnest(colors) %>%\n  #   \n  #   # Merge url and query parameters for the colors\n  #   mutate(url_color = glue(\"{url}?dwvar_{id}_pv_rahmenfarbe={colors}\")) %>%\n  #   select(-url) %>%\n  #   \n  #   # Use stringi to replace the last dash with the HTLM format of a dash (%2F)\n  #   # Only if there is a dash in the color column\n  #   mutate(url_color = ifelse(str_detect(colors, pattern = \"/\"),\n  #                             \n  #                             # if TRUE --> replace      \n  #                             stringi::stri_replace_last_fixed(url_color, \"/\", \"%2F\"),\n  #                             \n  #                             # ELSE --> take the original url\n  #                             url_color))\n  # \n  # bike_data_colors_tbl %>% glimpse()\n  # \n  # # Create function\n  # get_sizes <- function(url) {\n  #   \n  #   json <- url %>%\n  #     \n  #     read_html() %>%\n  #     \n  #     # Get all 'script nodes' and convert to char\n  #     html_nodes(css = \"script\") %>%\n  #     as.character() %>%\n  #     \n  #     # Select the node, that contains 'window.deptsfra'\n  #     str_subset(pattern = \"window.deptsfra\") %>%\n  #     \n  #     # remove the chars that do not belong to the json\n  #     # 1. replace at the beginning everything until the first \"{\" with \"\"\n  #     str_replace(\"^[^\\\\{]+\", \"\") %>%\n  #     # 2. replace at the end everything after the last \"}\" with \"\"\n  #     str_replace(\"[^\\\\}]+$\", \"\") %>%\n  #     \n  #     # Convert from json to an r object and pick the relevant values\n  #     fromJSON(flatten = T) %>%\n  #     purrr::pluck(\"productDetail\", \"variationAttributes\", \"values\", 2) %>%\n  #     \n  #     # select(id, value, available, availability)# %>%\n  #     select(id, value, availability.onlyXLeftNumber) %>%\n  #     \n  #     # Rename\n  #     rename(id_size = id) %>%\n  #     rename(size = value) %>%\n  #     rename(stock_availability = availability.onlyXLeftNumber) %>%\n  #     \n  #     # Conver to tibble\n  #     as_tibble()\n  #   \n  # }\n  # \n  # # Pull url vector\n  # bike_url_color_vec <- bike_data_colors_tbl %>% \n  #   pull(url_color)\n  # \n  # # Map\n  # bike_data_sizes_tbl <- bike_data_colors_tbl %>% \n  #   mutate(size = future_map(bike_url_color_vec, get_sizes))\n  # \n  # # Unnest\n  # bike_data_sizes_tbl <- bike_data_sizes_tbl %>% \n  #   unnest(size)\n  # \n  # saveRDS(bike_data_sizes_tbl, \"bike_data_sizes_tbl.rds\")"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html#api-call-and-data-extraction",
    "href": "content/01_journal/02_data_acquisition.html#api-call-and-data-extraction",
    "title": "Tidyverse",
    "section": "\n2.1 API call and data extraction",
    "text": "2.1 API call and data extraction\n\n# Challenge 2.1 ----\n\n# 1.0 LIBRARIES ----\n\nlibrary(tidyverse) # Main Package - Loads dplyr, purrr, etc.\nlibrary(rvest)     # HTML Hacking & Web Scraping\nlibrary(xopen)     # Quickly opening URLs\nlibrary(jsonlite)  # converts JSON files to R objects\nlibrary(glue)      # concatenate strings\nlibrary(stringi)   # character string/text processing\nlibrary(RSQLite)\nlibrary(httr)\n\n# Get api key from renviron file \napikey <- Sys.getenv('key')\n\n# store openweathermap city id for hamburg to variable\ncity_id <- 2911298\n\n# Call to openweathermao api \nweather <- GET(glue(\"http://api.openweathermap.org/data/2.5/forecast?id={city_id}&APPID={apikey}\"))\n\n# convert API response body\nweather_extracted <- rawToChar(weather$content)  %>% fromJSON()\nweather_extracted\n\n#> $cod\n#> [1] \"200\"\n#> \n#> $message\n#> [1] 0\n#> \n#> $cnt\n#> [1] 40\n#> \n#> $list\n#>            dt main.temp main.feels_like main.temp_min main.temp_max\n#> 1  1684800000    288.18          288.30        285.82        288.18\n#> 2  1684810800    287.53          287.61        286.62        287.53\n#> 3  1684821600    287.54          287.57        287.54        287.54\n#> 4  1684832400    285.91          285.70        285.91        285.91\n#> 5  1684843200    287.90          287.13        287.90        287.90\n#> 6  1684854000    287.22          286.12        287.22        287.22\n#> 7  1684864800    283.66          282.57        283.66        283.66\n#> 8  1684875600    280.76          278.25        280.76        280.76\n#> 9  1684886400    280.60          278.29        280.60        280.60\n#> 10 1684897200    281.38          278.62        281.38        281.38\n#> 11 1684908000    283.49          282.93        283.49        283.49\n#> 12 1684918800    285.25          284.60        285.25        285.25\n#> 13 1684929600    286.49          285.84        286.49        286.49\n#> 14 1684940400    289.92          289.11        289.92        289.92\n#> 15 1684951200    286.59          286.21        286.59        286.59\n#> 16 1684962000    282.83          281.71        282.83        282.83\n#> 17 1684972800    281.13          279.86        281.13        281.13\n#> 18 1684983600    280.24          278.84        280.24        280.24\n#> 19 1684994400    283.32          282.43        283.32        283.32\n#> 20 1685005200    288.61          287.41        288.61        288.61\n#> 21 1685016000    291.25          290.16        291.25        291.25\n#> 22 1685026800    290.74          289.76        290.74        290.74\n#> 23 1685037600    286.36          285.43        286.36        286.36\n#> 24 1685048400    280.44          278.14        280.44        280.44\n#> 25 1685059200    278.44          276.20        278.44        278.44\n#> 26 1685070000    278.01          275.73        278.01        278.01\n#> 27 1685080800    283.15          280.80        283.15        283.15\n#> 28 1685091600    287.04          285.79        287.04        287.04\n#> 29 1685102400    289.72          288.61        289.72        289.72\n#> 30 1685113200    289.73          288.72        289.73        289.73\n#> 31 1685124000    285.84          284.97        285.84        285.84\n#> 32 1685134800    280.22          278.52        280.22        280.22\n#> 33 1685145600    278.66          277.13        278.66        278.66\n#> 34 1685156400    277.72          276.67        277.72        277.72\n#> 35 1685167200    282.84          282.84        282.84        282.84\n#> 36 1685178000    288.46          287.35        288.46        288.46\n#> 37 1685188800    291.31          290.25        291.31        291.31\n#> 38 1685199600    291.56          290.58        291.56        291.56\n#> 39 1685210400    288.30          287.70        288.30        288.30\n#> 40 1685221200    282.78          282.22        282.78        282.78\n#>    main.pressure main.sea_level main.grnd_level main.humidity main.temp_kf\n#> 1           1013           1013            1009            98         2.36\n#> 2           1012           1012            1008            99         0.91\n#> 3           1012           1012            1009            97         0.00\n#> 4           1014           1014            1012            94         0.00\n#> 5           1016           1016            1014            65         0.00\n#> 6           1017           1017            1015            55         0.00\n#> 7           1019           1019            1016            69         0.00\n#> 8           1019           1019            1017            80         0.00\n#> 9           1019           1019            1016            89         0.00\n#> 10          1017           1017            1015            90         0.00\n#> 11          1018           1018            1016            90         0.00\n#> 12          1019           1019            1017            80         0.00\n#> 13          1020           1020            1018            75         0.00\n#> 14          1020           1020            1018            56         0.00\n#> 15          1021           1021            1018            85         0.00\n#> 16          1023           1023            1020            98         0.00\n#> 17          1023           1023            1020            98         0.00\n#> 18          1024           1024            1021            98         0.00\n#> 19          1025           1025            1022            78         0.00\n#> 20          1026           1026            1023            46         0.00\n#> 21          1025           1025            1023            40         0.00\n#> 22          1026           1026            1023            46         0.00\n#> 23          1027           1027            1024            65         0.00\n#> 24          1028           1028            1026            87         0.00\n#> 25          1029           1029            1026            95         0.00\n#> 26          1029           1029            1026            92         0.00\n#> 27          1030           1030            1028            67         0.00\n#> 28          1030           1030            1028            50         0.00\n#> 29          1030           1030            1027            45         0.00\n#> 30          1029           1029            1027            49         0.00\n#> 31          1030           1030            1027            69         0.00\n#> 32          1031           1031            1028            93         0.00\n#> 33          1031           1031            1028            97         0.00\n#> 34          1030           1030            1028            96         0.00\n#> 35          1030           1030            1027            73         0.00\n#> 36          1029           1029            1026            50         0.00\n#> 37          1027           1027            1025            41         0.00\n#> 38          1025           1025            1023            43         0.00\n#> 39          1025           1025            1022            70         0.00\n#> 40          1024           1024            1022            93         0.00\n#>                               weather all wind.speed wind.deg wind.gust\n#> 1          500, Rain, light rain, 10n  36       0.98      138      1.49\n#> 2          500, Rain, light rain, 10n  63       2.23      317      5.85\n#> 3   804, Clouds, overcast clouds, 04d  91       3.92      318      7.55\n#> 4   804, Clouds, overcast clouds, 04d 100       5.64      320     10.39\n#> 5   804, Clouds, overcast clouds, 04d 100       6.37      316      8.90\n#> 6   804, Clouds, overcast clouds, 04d 100       6.78      314      9.52\n#> 7   804, Clouds, overcast clouds, 04d  95       5.33      302      9.79\n#> 8     803, Clouds, broken clouds, 04n  63       3.94      287     10.72\n#> 9          500, Rain, light rain, 10n  68       3.51      262      9.92\n#> 10         500, Rain, light rain, 10n  99       4.77      269     10.80\n#> 11         500, Rain, light rain, 10d 100       4.05      300      7.60\n#> 12         500, Rain, light rain, 10d  99       2.53      323      3.65\n#> 13         500, Rain, light rain, 10d  99       2.62      301      3.11\n#> 14    803, Clouds, broken clouds, 04d  63       1.43      330      1.98\n#> 15         500, Rain, light rain, 10d  69       2.33      274      4.30\n#> 16         500, Rain, light rain, 10n  45       2.33      293      5.53\n#> 17 802, Clouds, scattered clouds, 03n  29       2.15      287      4.45\n#> 18       801, Clouds, few clouds, 02n  21       2.12      309      5.21\n#> 19 802, Clouds, scattered clouds, 03d  35       2.94      316      5.71\n#> 20         800, Clear, clear sky, 01d   4       3.71      324      4.98\n#> 21         800, Clear, clear sky, 01d   6       4.30      328      5.14\n#> 22         800, Clear, clear sky, 01d   5       5.09      331      6.11\n#> 23         800, Clear, clear sky, 01d   7       4.36      323      7.63\n#> 24 802, Clouds, scattered clouds, 03n  50       3.43      312     10.11\n#> 25 802, Clouds, scattered clouds, 03n  27       2.74      300     11.16\n#> 26         800, Clear, clear sky, 01n   0       2.68      297     10.59\n#> 27         800, Clear, clear sky, 01d   2       4.85      332      9.21\n#> 28    803, Clouds, broken clouds, 04d  59       4.20      333      5.99\n#> 29 802, Clouds, scattered clouds, 03d  36       4.28      328      5.92\n#> 30         800, Clear, clear sky, 01d   8       5.04      324      6.00\n#> 31         800, Clear, clear sky, 01d  10       4.26      315      6.76\n#> 32         800, Clear, clear sky, 01n   0       2.48      313      7.47\n#> 33         800, Clear, clear sky, 01n  10       1.98      332      3.86\n#> 34       801, Clouds, few clouds, 02n  12       1.45      339      1.46\n#> 35         800, Clear, clear sky, 01d   6       0.99      335      1.34\n#> 36         800, Clear, clear sky, 01d   1       1.23      344      1.44\n#> 37         800, Clear, clear sky, 01d   3       1.36       33      1.71\n#> 38         800, Clear, clear sky, 01d   4       2.16       22      2.04\n#> 39         800, Clear, clear sky, 01d   4       2.42       15      4.89\n#> 40         800, Clear, clear sky, 01n   5       1.65       58      1.66\n#>    visibility  pop   3h pod              dt_txt\n#> 1       10000 1.00 0.65   n 2023-05-23 00:00:00\n#> 2        8607 0.36 0.20   n 2023-05-23 03:00:00\n#> 3       10000 0.04   NA   d 2023-05-23 06:00:00\n#> 4       10000 0.00   NA   d 2023-05-23 09:00:00\n#> 5       10000 0.00   NA   d 2023-05-23 12:00:00\n#> 6       10000 0.00   NA   d 2023-05-23 15:00:00\n#> 7       10000 0.00   NA   d 2023-05-23 18:00:00\n#> 8       10000 0.00   NA   n 2023-05-23 21:00:00\n#> 9       10000 0.20 0.18   n 2023-05-24 00:00:00\n#> 10      10000 0.40 0.27   n 2023-05-24 03:00:00\n#> 11      10000 0.42 0.23   d 2023-05-24 06:00:00\n#> 12      10000 0.46 0.41   d 2023-05-24 09:00:00\n#> 13      10000 0.52 0.21   d 2023-05-24 12:00:00\n#> 14      10000 0.24   NA   d 2023-05-24 15:00:00\n#> 15      10000 0.20 0.13   d 2023-05-24 18:00:00\n#> 16      10000 0.20 0.10   n 2023-05-24 21:00:00\n#> 17      10000 0.00   NA   n 2023-05-25 00:00:00\n#> 18      10000 0.00   NA   n 2023-05-25 03:00:00\n#> 19      10000 0.00   NA   d 2023-05-25 06:00:00\n#> 20      10000 0.00   NA   d 2023-05-25 09:00:00\n#> 21      10000 0.00   NA   d 2023-05-25 12:00:00\n#> 22      10000 0.00   NA   d 2023-05-25 15:00:00\n#> 23      10000 0.00   NA   d 2023-05-25 18:00:00\n#> 24      10000 0.00   NA   n 2023-05-25 21:00:00\n#> 25      10000 0.00   NA   n 2023-05-26 00:00:00\n#> 26      10000 0.00   NA   n 2023-05-26 03:00:00\n#> 27      10000 0.00   NA   d 2023-05-26 06:00:00\n#> 28      10000 0.00   NA   d 2023-05-26 09:00:00\n#> 29      10000 0.00   NA   d 2023-05-26 12:00:00\n#> 30      10000 0.00   NA   d 2023-05-26 15:00:00\n#> 31      10000 0.00   NA   d 2023-05-26 18:00:00\n#> 32      10000 0.00   NA   n 2023-05-26 21:00:00\n#> 33      10000 0.00   NA   n 2023-05-27 00:00:00\n#> 34      10000 0.00   NA   n 2023-05-27 03:00:00\n#> 35      10000 0.00   NA   d 2023-05-27 06:00:00\n#> 36      10000 0.00   NA   d 2023-05-27 09:00:00\n#> 37      10000 0.00   NA   d 2023-05-27 12:00:00\n#> 38      10000 0.00   NA   d 2023-05-27 15:00:00\n#> 39      10000 0.00   NA   d 2023-05-27 18:00:00\n#> 40      10000 0.00   NA   n 2023-05-27 21:00:00\n#> \n#> $city\n#> $city$id\n#> [1] 2911298\n#> \n#> $city$name\n#> [1] \"Hamburg\"\n#> \n#> $city$coord\n#> $city$coord$lat\n#> [1] 53.55\n#> \n#> $city$coord$lon\n#> [1] 10\n#> \n#> \n#> $city$country\n#> [1] \"DE\"\n#> \n#> $city$population\n#> [1] 0\n#> \n#> $city$timezone\n#> [1] 7200\n#> \n#> $city$sunrise\n#> [1] 1684724969\n#> \n#> $city$sunset\n#> [1] 1684783445\n\n# extract wind speed data from response\nwindspeed <- weather_extracted$list$wind$speed\nwindspeed  %>% glimpse()\n\n#>  num [1:40] 0.98 2.23 3.92 5.64 6.37 6.78 5.33 3.94 3.51 4.77 ...\n\n# extract city name from response\ncity_name <- weather_extracted$city$name\n\n# hour list\nhours <- seq(0,120-1,3)\n\n# combine hour list and windspeed data\nwindspeed_tbl <- tibble(hours, windspeed)\nwindspeed_tbl %>% glimpse()\n\n#> Rows: 40\n#> Columns: 2\n#> $ hours     <dbl> 0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, …\n#> $ windspeed <dbl> 0.98, 2.23, 3.92, 5.64, 6.37, 6.78, 5.33, 3.94, 3.51, 4.77, …"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html#visualization",
    "href": "content/01_journal/02_data_acquisition.html#visualization",
    "title": "Tidyverse",
    "section": "\n2.2 Visualization",
    "text": "2.2 Visualization\n\n# plot results\nwindspeed_tbl %>%\n  \n  # Set up x, y, fill\n  ggplot(aes(x = hours, y = windspeed)) +\n  \n  # Geometries\n  geom_col() + # Run up to here to get a stacked bar plot\n  \n  # Formatting\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \"m/s\")) +\n  labs(\n    title = glue(\"Wind speed in {city_name}\"),\n    subtitle = \"Forecast for the next 5 days in 3 hour intervals\",\n    x = \"Nr. of hours into the future\", \n    y = \"Wind speed in m/s\")"
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "(Code mainly from startupengineer templates)\n\n# Data Science at TUHH ------------------------------------------------------\n# SALES ANALYSIS ----\n\n# 1.0 Load libraries ----\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(readxl)\n\n\n# 2.0 Importing Files ----\n# A good convention is to use the file name and suffix it with tbl for the data structure tibble\nbikes_tbl      <- read_excel(path = \"./../../00_data/01_bike_sales/01_raw_data/bikes.xlsx\")\norderlines_tbl <- read_excel(\"./../../00_data/01_bike_sales/01_raw_data/orderlines.xlsx\")\n\n#> New names:\n#> • `` -> `...1`\n\n# Not necessary for this analysis, but for the sake of completeness\nbikeshops_tbl  <- read_excel(\"./../../00_data/01_bike_sales/01_raw_data/bikeshops.xlsx\")\n\n# 3.0 Examining Data ----\n# Method 1: Print it to the console\n# orderlines_tbl\n\n# Method 2: Clicking on the file in the environment tab (or run View(orderlines_tbl)) There you can play around with the filter.\n# View(orderlines_tbl)\n\n# Method 3: glimpse() function. Especially helpful for wide data (data with many columns)\nglimpse(orderlines_tbl)\n\n#> Rows: 15,644\n#> Columns: 7\n#> $ ...1        <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"…\n#> $ order.id    <dbl> 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7…\n#> $ order.line  <dbl> 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2…\n#> $ order.date  <dttm> 2015-01-07, 2015-01-07, 2015-01-10, 2015-01-10, 2015-01-1…\n#> $ customer.id <dbl> 2, 2, 10, 10, 6, 6, 6, 6, 6, 22, 8, 8, 8, 8, 16, 16, 16, 1…\n#> $ product.id  <dbl> 2681, 2411, 2629, 2137, 2367, 1973, 2422, 2655, 2247, 2408…\n#> $ quantity    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1…\n\n# 4.0 Joining Data ----\n# by automatically detecting a common column, if any ...\n# left_join(orderlines_tbl, bikes_tbl)\n\n# If the data has no common column name, you can provide each column name in the \"by\" argument. For example, by = c(\"a\" = \"b\") will match x.a to y.b. The order of the columns has to match the order of the tibbles).\nleft_join(orderlines_tbl, bikes_tbl, by = c(\"product.id\" = \"bike.id\"))\n\n\n\n  \n\n\n# Chaining commands with the pipe and assigning it to order_items_joined_tbl\nbike_orderlines_joined_tbl <- orderlines_tbl %>%\n  left_join(bikes_tbl, by = c(\"product.id\" = \"bike.id\")) %>%\n  left_join(bikeshops_tbl, by = c(\"customer.id\" = \"bikeshop.id\"))\n\n# Examine the results with glimpse()\nbike_orderlines_joined_tbl %>% glimpse()\n\n#> Rows: 15,644\n#> Columns: 19\n#> $ ...1           <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\"…\n#> $ order.id       <dbl> 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7…\n#> $ order.line     <dbl> 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1…\n#> $ order.date     <dttm> 2015-01-07, 2015-01-07, 2015-01-10, 2015-01-10, 2015-0…\n#> $ customer.id    <dbl> 2, 2, 10, 10, 6, 6, 6, 6, 6, 22, 8, 8, 8, 8, 16, 16, 16…\n#> $ product.id     <dbl> 2681, 2411, 2629, 2137, 2367, 1973, 2422, 2655, 2247, 2…\n#> $ quantity       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1…\n#> $ model          <chr> \"Spectral CF 7 WMN\", \"Ultimate CF SLX Disc 8.0 ETAP\", \"…\n#> $ model.year     <dbl> 2021, 2020, 2021, 2019, 2020, 2020, 2020, 2021, 2020, 2…\n#> $ frame.material <chr> \"carbon\", \"carbon\", \"carbon\", \"carbon\", \"aluminium\", \"c…\n#> $ weight         <dbl> 13.80, 7.44, 14.06, 8.80, 11.50, 8.80, 8.20, 8.85, 14.4…\n#> $ price          <dbl> 3119, 5359, 2729, 1749, 1219, 1359, 2529, 1559, 3899, 6…\n#> $ category       <chr> \"Mountain - Trail - Spectral\", \"Road - Race - Ultimate\"…\n#> $ gender         <chr> \"female\", \"unisex\", \"unisex\", \"unisex\", \"unisex\", \"unis…\n#> $ url            <chr> \"https://www.canyon.com/en-de/mountain-bikes/trail-bike…\n#> $ name           <chr> \"AlexandeRad\", \"AlexandeRad\", \"WITT-RAD\", \"WITT-RAD\", \"…\n#> $ location       <chr> \"Hamburg, Hamburg\", \"Hamburg, Hamburg\", \"Bremen, Bremen…\n#> $ lat            <dbl> 53.57532, 53.57532, 53.07379, 53.07379, 48.78234, 48.78…\n#> $ lng            <dbl> 10.015340, 10.015340, 8.826754, 8.826754, 9.180819, 9.1…\n\n# 5.0 Wrangling Data ----\n# print all unique entries, that start with Mountain\nbike_orderlines_joined_tbl %>% \n  select(category) %>%\n  filter(str_detect(category, \"^Mountain\")) %>% \n  unique()\n\n\n\n  \n\n\n# All actions are chained with the pipe already. You can perform each step separately and use glimpse() or View() to validate your code. Store the result in a variable at the end of the steps.\nbike_orderlines_wrangled_tbl <- bike_orderlines_joined_tbl %>%\n  # 5.1 Separate category name\n  separate(col    = category,\n           into   = c(\"category.1\", \"category.2\", \"category.3\"),\n           sep    = \" - \") %>%\n  \n  # 5.2 Add the total price (price * quantity) \n  # Add a column to a tibble that uses a formula-style calculation of other columns\n  mutate(total.price = price * quantity) %>%\n  \n  # 5.3 Optional: Reorganize. Using select to grab or remove unnecessary columns\n  # 5.3.1 by exact column name\n  select(-...1, -gender) %>%\n  \n  # 5.3.2 by a pattern\n  # You can use the select_helpers to define patterns. \n  # Type ?ends_with and click on Select helpers in the documentation\n  select(-ends_with(\".id\")) %>%\n  \n  # 5.3.3 Actually we need the column \"order.id\". Let's bind it back to the data\n  bind_cols(bike_orderlines_joined_tbl %>% select(order.id)) %>% \n  \n  # 5.3.4 You can reorder the data by selecting the columns in your desired order.\n  # You can use select_helpers like contains() or everything()\n  select(order.id, contains(\"order\"), contains(\"model\"), contains(\"category\"),\n         price, quantity, total.price,\n         everything()) %>%\n  \n  # 5.4 Rename columns because we actually wanted underscores instead of the dots\n  # (one at the time vs. multiple at once)\n  rename(bikeshop = name) %>%\n  set_names(names(.) %>% str_replace_all(\"\\\\.\", \"_\"))\n\n\n# 6.0 Business Insights ----\n# 6.1 Sales by Year ----\n\n# Step 1 - Manipulate\nsales_by_year_tbl <- bike_orderlines_wrangled_tbl %>%\n  \n  # Select columns\n  select(order_date, total_price) %>%\n  \n  # Add year column\n  mutate(year = year(order_date)) %>%\n  \n  # Grouping by year and summarizing sales\n  group_by(year) %>% \n  summarize(sales = sum(total_price)) %>%\n  \n  # Optional: Add a column that turns the numbers into a currency format \n  # (makes it in the plot optically more appealing)\n  # mutate(sales_text = scales::dollar(sales)) <- Works for dollar values\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n\nsales_by_year_tbl\n\n\n\n  \n\n\n# Step 2 - Visualize\nsales_by_year_tbl %>%\n  \n  # Setup canvas with the columns year (x-axis) and sales (y-axis)\n  ggplot(aes(x = year, y = sales)) +\n  \n  # Geometries\n  geom_col(fill = \"#2DC6D6\") + # Use geom_col for a bar plot\n  geom_label(aes(label = sales_text)) + # Adding labels to the bars\n  geom_smooth(method = \"lm\", se = FALSE) + # Adding a trendline\n  \n  # Formatting\n  # scale_y_continuous(labels = scales::dollar) + # Change the y-axis. \n  # Again, we have to adjust it for euro values\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title    = \"Revenue by year\",\n    subtitle = \"Upward Trend\",\n    x = \"\", # Override defaults for x and y\n    y = \"Revenue\"\n  )\n\n#> `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n# 6.2 Sales by Year and Category 2 ----\n\n# Step 1 - Manipulate\nsales_by_year_cat_1_tbl <- bike_orderlines_wrangled_tbl %>%\n  \n  # Select columns and add a year\n  select(order_date, total_price, category_1) %>%\n  mutate(year = year(order_date)) %>%\n  \n  # Group by and summarize year and main catgegory\n  group_by(year, category_1) %>%\n  summarise(sales = sum(total_price)) %>%\n  ungroup() %>%\n  \n  # Format $ Text\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n\n#> `summarise()` has grouped output by 'year'. You can override using the\n#> `.groups` argument.\n\nsales_by_year_cat_1_tbl  \n\n\n\n  \n\n\n# Step 2 - Visualize\nsales_by_year_cat_1_tbl %>%\n  \n  # Set up x, y, fill\n  ggplot(aes(x = year, y = sales, fill = category_1)) +\n  \n  # Geometries\n  geom_col() + # Run up to here to get a stacked bar plot\n  \n  # Facet\n  facet_wrap(~ category_1) +\n  \n  # Formatting\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title = \"Revenue by year and main category\",\n    subtitle = \"Each product category has an upward trend\",\n    fill = \"Main category\" # Changes the legend name\n  )\n\n\n\n\n\n\n# 7.0 Writing Files ----\n\n# 7.1 Excel ----\n# install.packages(\"writexl\")\nlibrary(\"writexl\")\nbike_orderlines_wrangled_tbl %>%\n  write_xlsx(\"./../../00_data/01_bike_sales/02_wrangled_data/bike_orderlines.xlsx\")\n\n# 7.2 CSV ----\nbike_orderlines_wrangled_tbl %>% \n  write_csv(\"./../../00_data/01_bike_sales/02_wrangled_data/bike_orderlines.csv\")\n\n# 7.3 RDS ----\nbike_orderlines_wrangled_tbl %>% \n  write_rds(\"./../../00_data/01_bike_sales/02_wrangled_data/bike_orderlines.rds\")"
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#part-1---revenue-by-state",
    "href": "content/01_journal/01_tidyverse.html#part-1---revenue-by-state",
    "title": "Tidyverse",
    "section": "\n2.1 Part 1 - Revenue by state",
    "text": "2.1 Part 1 - Revenue by state\n\n# 8.0 Challenge ----\n# 8.1 Manipulate Data\n# Separate city and state\nbike_orderlines_wrangled_tbl <- bike_orderlines_wrangled_tbl %>%\n  separate(col    = location,\n           into   = c(\"city\", \"state\"),\n           sep    = \", \")\n  \n# Extract state data\nsales_by_state_tbl <- bike_orderlines_wrangled_tbl %>%\n  \n  # Select columns and add a year\n  select(state, total_price) %>%\n  \n  # Group by and summarize year and main catgegory\n  group_by(state) %>%\n  summarise(sales = sum(total_price)) %>%\n  \n  # Format $ Text\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n\n  # extract state with highes sales \n  highest_value_state <- sales_by_state_tbl[which.max( sales_by_state_tbl$sales ),1]\n\n\n# 8.2 Visualize Data \nsales_by_state_tbl %>%\n  \n  # Set up x, y, fill\n  ggplot(aes(x = state, y = sales)) +\n  \n  # Geometries\n  geom_col() + # Run up to here to get a stacked bar plot\n  \n  # Formatting\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title = \"Revenue by state\",\n    subtitle = paste0(\"The state with the highest revenue is \", highest_value_state),\n    x = \"State\",\n    y = \"Revenue\"\n  )"
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#part-2---revenue-by-year-and-state",
    "href": "content/01_journal/01_tidyverse.html#part-2---revenue-by-year-and-state",
    "title": "Tidyverse",
    "section": "\n2.2 Part 2 - Revenue by year and state",
    "text": "2.2 Part 2 - Revenue by year and state\n\n# 8.2 Sales by Year and Location ----\n\n# # Step 1 - Manipulate\nsales_by_year_state_tbl <- bike_orderlines_wrangled_tbl %>%\n  \n  # Select columns and add a year\n  select(order_date, total_price, state) %>%\n  mutate(year = year(order_date)) %>%\n  \n  # Group by and summarize year and main catgegory\n  group_by(year, state) %>%\n  summarise(sales = sum(total_price)) %>%\n  ungroup() %>%\n  \n  # Format $ Text\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n\n#> `summarise()` has grouped output by 'year'. You can override using the\n#> `.groups` argument.\n\nsales_by_year_state_tbl  \n\n\n\n  \n\n\n\n\n# Step 2 - Visualize\nsales_by_year_state_tbl %>%\n  \n  # Set up x, y, fill\n  ggplot(aes(x = year, y = sales, fill = state)) +\n  \n  # Geometries\n  geom_col() + # Run up to here to get a stacked bar plot\n  \n  # Facet\n  facet_wrap(~ state) +\n  \n  # Formatting\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title = \"Revenue by year and state\",\n    fill = \"Main category\" # Changes the legend name\n  )"
  }
]