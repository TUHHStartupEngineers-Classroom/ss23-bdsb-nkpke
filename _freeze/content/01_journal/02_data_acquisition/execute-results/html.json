{
  "hash": "3a85acc0dad306a2683d0d75f5a034f4",
  "result": {
    "markdown": "---\ntitle: \"Tidyverse\"\nauthor: \"Nis Köpke\"\n---\n\n\n<details>\n  <summary>Expand Data Acquisition Preparation</summary>\n# Data Acquisition Preparation\n(Code mainly from startupengineer templates)\n\nBusiness code commented out because of website changes.\n\n\n::: {.cell hash='02_data_acquisition_cache/html/unnamed-chunk-1_f69d8c3347bc29763a606c48663ee3e1'}\n\n```{.r .cell-code}\n# WEBSCRAPING ----\n\n# 1.0 LIBRARIES ----\n\nlibrary(tidyverse) # Main Package - Loads dplyr, purrr, etc.\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(rvest)     # HTML Hacking & Web Scraping\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> \n#> Attaching package: 'rvest'\n#> \n#> The following object is masked from 'package:readr':\n#> \n#>     guess_encoding\n```\n:::\n\n```{.r .cell-code}\nlibrary(xopen)     # Quickly opening URLs\nlibrary(jsonlite)  # converts JSON files to R objects\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> \n#> Attaching package: 'jsonlite'\n#> \n#> The following object is masked from 'package:purrr':\n#> \n#>     flatten\n```\n:::\n\n```{.r .cell-code}\nlibrary(glue)      # concatenate strings\nlibrary(stringi)   # character string/text processing\nlibrary(RSQLite)\nlibrary(httr)\n\n# Import and list tables\ncon <- RSQLite::dbConnect(drv = SQLite(), dbname = \"./../../00_data/02_chinook/Chinook_Sqlite.sqlite\")\ndbListTables(con)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  [1] \"Album\"         \"Artist\"        \"Customer\"      \"Employee\"     \n#>  [5] \"Genre\"         \"Invoice\"       \"InvoiceLine\"   \"MediaType\"    \n#>  [9] \"Playlist\"      \"PlaylistTrack\" \"Track\"\n```\n:::\n\n```{.r .cell-code}\n# Look at a table\ntbl(con, \"Album\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"AlbumId\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Title\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ArtistId\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"For Those About To Rock We Salute You\",\"3\":\"1\"},{\"1\":\"2\",\"2\":\"Balls to the Wall\",\"3\":\"2\"},{\"1\":\"3\",\"2\":\"Restless and Wild\",\"3\":\"2\"},{\"1\":\"4\",\"2\":\"Let There Be Rock\",\"3\":\"1\"},{\"1\":\"5\",\"2\":\"Big Ones\",\"3\":\"3\"},{\"1\":\"6\",\"2\":\"Jagged Little Pill\",\"3\":\"4\"},{\"1\":\"7\",\"2\":\"Facelift\",\"3\":\"5\"},{\"1\":\"8\",\"2\":\"Warner 25 Anos\",\"3\":\"6\"},{\"1\":\"9\",\"2\":\"Plays Metallica By Four Cellos\",\"3\":\"7\"},{\"1\":\"10\",\"2\":\"Audioslave\",\"3\":\"8\"},{\"1\":\"11\",\"2\":\"Out Of Exile\",\"3\":\"8\"},{\"1\":\"12\",\"2\":\"BackBeat Soundtrack\",\"3\":\"9\"},{\"1\":\"13\",\"2\":\"The Best Of Billy Cobham\",\"3\":\"10\"},{\"1\":\"14\",\"2\":\"Alcohol Fueled Brewtality Live! [Disc 1]\",\"3\":\"11\"},{\"1\":\"15\",\"2\":\"Alcohol Fueled Brewtality Live! [Disc 2]\",\"3\":\"11\"},{\"1\":\"16\",\"2\":\"Black Sabbath\",\"3\":\"12\"},{\"1\":\"17\",\"2\":\"Black Sabbath Vol. 4 (Remaster)\",\"3\":\"12\"},{\"1\":\"18\",\"2\":\"Body Count\",\"3\":\"13\"},{\"1\":\"19\",\"2\":\"Chemical Wedding\",\"3\":\"14\"},{\"1\":\"20\",\"2\":\"The Best Of Buddy Guy - The Millenium Collection\",\"3\":\"15\"},{\"1\":\"21\",\"2\":\"Prenda Minha\",\"3\":\"16\"},{\"1\":\"22\",\"2\":\"Sozinho Remix Ao Vivo\",\"3\":\"16\"},{\"1\":\"23\",\"2\":\"Minha Historia\",\"3\":\"17\"},{\"1\":\"24\",\"2\":\"Afrociberdelia\",\"3\":\"18\"},{\"1\":\"25\",\"2\":\"Da Lama Ao Caos\",\"3\":\"18\"},{\"1\":\"26\",\"2\":\"Acústico MTV [Live]\",\"3\":\"19\"},{\"1\":\"27\",\"2\":\"Cidade Negra - Hits\",\"3\":\"19\"},{\"1\":\"28\",\"2\":\"Na Pista\",\"3\":\"20\"},{\"1\":\"29\",\"2\":\"Axé Bahia 2001\",\"3\":\"21\"},{\"1\":\"30\",\"2\":\"BBC Sessions [Disc 1] [Live]\",\"3\":\"22\"},{\"1\":\"31\",\"2\":\"Bongo Fury\",\"3\":\"23\"},{\"1\":\"32\",\"2\":\"Carnaval 2001\",\"3\":\"21\"},{\"1\":\"33\",\"2\":\"Chill: Brazil (Disc 1)\",\"3\":\"24\"},{\"1\":\"34\",\"2\":\"Chill: Brazil (Disc 2)\",\"3\":\"6\"},{\"1\":\"35\",\"2\":\"Garage Inc. (Disc 1)\",\"3\":\"50\"},{\"1\":\"36\",\"2\":\"Greatest Hits II\",\"3\":\"51\"},{\"1\":\"37\",\"2\":\"Greatest Kiss\",\"3\":\"52\"},{\"1\":\"38\",\"2\":\"Heart of the Night\",\"3\":\"53\"},{\"1\":\"39\",\"2\":\"International Superhits\",\"3\":\"54\"},{\"1\":\"40\",\"2\":\"Into The Light\",\"3\":\"55\"},{\"1\":\"41\",\"2\":\"Meus Momentos\",\"3\":\"56\"},{\"1\":\"42\",\"2\":\"Minha História\",\"3\":\"57\"},{\"1\":\"43\",\"2\":\"MK III The Final Concerts [Disc 1]\",\"3\":\"58\"},{\"1\":\"44\",\"2\":\"Physical Graffiti [Disc 1]\",\"3\":\"22\"},{\"1\":\"45\",\"2\":\"Sambas De Enredo 2001\",\"3\":\"21\"},{\"1\":\"46\",\"2\":\"Supernatural\",\"3\":\"59\"},{\"1\":\"47\",\"2\":\"The Best of Ed Motta\",\"3\":\"37\"},{\"1\":\"48\",\"2\":\"The Essential Miles Davis [Disc 1]\",\"3\":\"68\"},{\"1\":\"49\",\"2\":\"The Essential Miles Davis [Disc 2]\",\"3\":\"68\"},{\"1\":\"50\",\"2\":\"The Final Concerts (Disc 2)\",\"3\":\"58\"},{\"1\":\"51\",\"2\":\"Up An' Atom\",\"3\":\"69\"},{\"1\":\"52\",\"2\":\"Vinícius De Moraes - Sem Limite\",\"3\":\"70\"},{\"1\":\"53\",\"2\":\"Vozes do MPB\",\"3\":\"21\"},{\"1\":\"54\",\"2\":\"Chronicle, Vol. 1\",\"3\":\"76\"},{\"1\":\"55\",\"2\":\"Chronicle, Vol. 2\",\"3\":\"76\"},{\"1\":\"56\",\"2\":\"Cássia Eller - Coleção Sem Limite [Disc 2]\",\"3\":\"77\"},{\"1\":\"57\",\"2\":\"Cássia Eller - Sem Limite [Disc 1]\",\"3\":\"77\"},{\"1\":\"58\",\"2\":\"Come Taste The Band\",\"3\":\"58\"},{\"1\":\"59\",\"2\":\"Deep Purple In Rock\",\"3\":\"58\"},{\"1\":\"60\",\"2\":\"Fireball\",\"3\":\"58\"},{\"1\":\"61\",\"2\":\"Knocking at Your Back Door: The Best Of Deep Purple in the 80's\",\"3\":\"58\"},{\"1\":\"62\",\"2\":\"Machine Head\",\"3\":\"58\"},{\"1\":\"63\",\"2\":\"Purpendicular\",\"3\":\"58\"},{\"1\":\"64\",\"2\":\"Slaves And Masters\",\"3\":\"58\"},{\"1\":\"65\",\"2\":\"Stormbringer\",\"3\":\"58\"},{\"1\":\"66\",\"2\":\"The Battle Rages On\",\"3\":\"58\"},{\"1\":\"67\",\"2\":\"Vault: Def Leppard's Greatest Hits\",\"3\":\"78\"},{\"1\":\"68\",\"2\":\"Outbreak\",\"3\":\"79\"},{\"1\":\"69\",\"2\":\"Djavan Ao Vivo - Vol. 02\",\"3\":\"80\"},{\"1\":\"70\",\"2\":\"Djavan Ao Vivo - Vol. 1\",\"3\":\"80\"},{\"1\":\"71\",\"2\":\"Elis Regina-Minha História\",\"3\":\"41\"},{\"1\":\"72\",\"2\":\"The Cream Of Clapton\",\"3\":\"81\"},{\"1\":\"73\",\"2\":\"Unplugged\",\"3\":\"81\"},{\"1\":\"74\",\"2\":\"Album Of The Year\",\"3\":\"82\"},{\"1\":\"75\",\"2\":\"Angel Dust\",\"3\":\"82\"},{\"1\":\"76\",\"2\":\"King For A Day Fool For A Lifetime\",\"3\":\"82\"},{\"1\":\"77\",\"2\":\"The Real Thing\",\"3\":\"82\"},{\"1\":\"78\",\"2\":\"Deixa Entrar\",\"3\":\"83\"},{\"1\":\"79\",\"2\":\"In Your Honor [Disc 1]\",\"3\":\"84\"},{\"1\":\"80\",\"2\":\"In Your Honor [Disc 2]\",\"3\":\"84\"},{\"1\":\"81\",\"2\":\"One By One\",\"3\":\"84\"},{\"1\":\"82\",\"2\":\"The Colour And The Shape\",\"3\":\"84\"},{\"1\":\"83\",\"2\":\"My Way: The Best Of Frank Sinatra [Disc 1]\",\"3\":\"85\"},{\"1\":\"84\",\"2\":\"Roda De Funk\",\"3\":\"86\"},{\"1\":\"85\",\"2\":\"As Canções de Eu Tu Eles\",\"3\":\"27\"},{\"1\":\"86\",\"2\":\"Quanta Gente Veio Ver (Live)\",\"3\":\"27\"},{\"1\":\"87\",\"2\":\"Quanta Gente Veio ver--Bônus De Carnaval\",\"3\":\"27\"},{\"1\":\"88\",\"2\":\"Faceless\",\"3\":\"87\"},{\"1\":\"89\",\"2\":\"American Idiot\",\"3\":\"54\"},{\"1\":\"90\",\"2\":\"Appetite for Destruction\",\"3\":\"88\"},{\"1\":\"91\",\"2\":\"Use Your Illusion I\",\"3\":\"88\"},{\"1\":\"92\",\"2\":\"Use Your Illusion II\",\"3\":\"88\"},{\"1\":\"93\",\"2\":\"Blue Moods\",\"3\":\"89\"},{\"1\":\"94\",\"2\":\"A Matter of Life and Death\",\"3\":\"90\"},{\"1\":\"95\",\"2\":\"A Real Dead One\",\"3\":\"90\"},{\"1\":\"96\",\"2\":\"A Real Live One\",\"3\":\"90\"},{\"1\":\"97\",\"2\":\"Brave New World\",\"3\":\"90\"},{\"1\":\"98\",\"2\":\"Dance Of Death\",\"3\":\"90\"},{\"1\":\"99\",\"2\":\"Fear Of The Dark\",\"3\":\"90\"},{\"1\":\"100\",\"2\":\"Iron Maiden\",\"3\":\"90\"},{\"1\":\"101\",\"2\":\"Killers\",\"3\":\"90\"},{\"1\":\"102\",\"2\":\"Live After Death\",\"3\":\"90\"},{\"1\":\"103\",\"2\":\"Live At Donington 1992 (Disc 1)\",\"3\":\"90\"},{\"1\":\"104\",\"2\":\"Live At Donington 1992 (Disc 2)\",\"3\":\"90\"},{\"1\":\"105\",\"2\":\"No Prayer For The Dying\",\"3\":\"90\"},{\"1\":\"106\",\"2\":\"Piece Of Mind\",\"3\":\"90\"},{\"1\":\"107\",\"2\":\"Powerslave\",\"3\":\"90\"},{\"1\":\"108\",\"2\":\"Rock In Rio [CD1]\",\"3\":\"90\"},{\"1\":\"109\",\"2\":\"Rock In Rio [CD2]\",\"3\":\"90\"},{\"1\":\"110\",\"2\":\"Seventh Son of a Seventh Son\",\"3\":\"90\"},{\"1\":\"111\",\"2\":\"Somewhere in Time\",\"3\":\"90\"},{\"1\":\"112\",\"2\":\"The Number of The Beast\",\"3\":\"90\"},{\"1\":\"113\",\"2\":\"The X Factor\",\"3\":\"90\"},{\"1\":\"114\",\"2\":\"Virtual XI\",\"3\":\"90\"},{\"1\":\"115\",\"2\":\"Sex Machine\",\"3\":\"91\"},{\"1\":\"116\",\"2\":\"Emergency On Planet Earth\",\"3\":\"92\"},{\"1\":\"117\",\"2\":\"Synkronized\",\"3\":\"92\"},{\"1\":\"118\",\"2\":\"The Return Of The Space Cowboy\",\"3\":\"92\"},{\"1\":\"119\",\"2\":\"Get Born\",\"3\":\"93\"},{\"1\":\"120\",\"2\":\"Are You Experienced?\",\"3\":\"94\"},{\"1\":\"121\",\"2\":\"Surfing with the Alien (Remastered)\",\"3\":\"95\"},{\"1\":\"122\",\"2\":\"Jorge Ben Jor 25 Anos\",\"3\":\"46\"},{\"1\":\"123\",\"2\":\"Jota Quest-1995\",\"3\":\"96\"},{\"1\":\"124\",\"2\":\"Cafezinho\",\"3\":\"97\"},{\"1\":\"125\",\"2\":\"Living After Midnight\",\"3\":\"98\"},{\"1\":\"126\",\"2\":\"Unplugged [Live]\",\"3\":\"52\"},{\"1\":\"127\",\"2\":\"BBC Sessions [Disc 2] [Live]\",\"3\":\"22\"},{\"1\":\"128\",\"2\":\"Coda\",\"3\":\"22\"},{\"1\":\"129\",\"2\":\"Houses Of The Holy\",\"3\":\"22\"},{\"1\":\"130\",\"2\":\"In Through The Out Door\",\"3\":\"22\"},{\"1\":\"131\",\"2\":\"IV\",\"3\":\"22\"},{\"1\":\"132\",\"2\":\"Led Zeppelin I\",\"3\":\"22\"},{\"1\":\"133\",\"2\":\"Led Zeppelin II\",\"3\":\"22\"},{\"1\":\"134\",\"2\":\"Led Zeppelin III\",\"3\":\"22\"},{\"1\":\"135\",\"2\":\"Physical Graffiti [Disc 2]\",\"3\":\"22\"},{\"1\":\"136\",\"2\":\"Presence\",\"3\":\"22\"},{\"1\":\"137\",\"2\":\"The Song Remains The Same (Disc 1)\",\"3\":\"22\"},{\"1\":\"138\",\"2\":\"The Song Remains The Same (Disc 2)\",\"3\":\"22\"},{\"1\":\"139\",\"2\":\"A TempestadeTempestade Ou O Livro Dos Dias\",\"3\":\"99\"},{\"1\":\"140\",\"2\":\"Mais Do Mesmo\",\"3\":\"99\"},{\"1\":\"141\",\"2\":\"Greatest Hits\",\"3\":\"100\"},{\"1\":\"142\",\"2\":\"Lulu Santos - RCA 100 Anos De Música - Álbum 01\",\"3\":\"101\"},{\"1\":\"143\",\"2\":\"Lulu Santos - RCA 100 Anos De Música - Álbum 02\",\"3\":\"101\"},{\"1\":\"144\",\"2\":\"Misplaced Childhood\",\"3\":\"102\"},{\"1\":\"145\",\"2\":\"Barulhinho Bom\",\"3\":\"103\"},{\"1\":\"146\",\"2\":\"Seek And Shall Find: More Of The Best (1963-1981)\",\"3\":\"104\"},{\"1\":\"147\",\"2\":\"The Best Of Men At Work\",\"3\":\"105\"},{\"1\":\"148\",\"2\":\"Black Album\",\"3\":\"50\"},{\"1\":\"149\",\"2\":\"Garage Inc. (Disc 2)\",\"3\":\"50\"},{\"1\":\"150\",\"2\":\"Kill 'Em All\",\"3\":\"50\"},{\"1\":\"151\",\"2\":\"Load\",\"3\":\"50\"},{\"1\":\"152\",\"2\":\"Master Of Puppets\",\"3\":\"50\"},{\"1\":\"153\",\"2\":\"ReLoad\",\"3\":\"50\"},{\"1\":\"154\",\"2\":\"Ride The Lightning\",\"3\":\"50\"},{\"1\":\"155\",\"2\":\"St. Anger\",\"3\":\"50\"},{\"1\":\"156\",\"2\":\"...And Justice For All\",\"3\":\"50\"},{\"1\":\"157\",\"2\":\"Miles Ahead\",\"3\":\"68\"},{\"1\":\"158\",\"2\":\"Milton Nascimento Ao Vivo\",\"3\":\"42\"},{\"1\":\"159\",\"2\":\"Minas\",\"3\":\"42\"},{\"1\":\"160\",\"2\":\"Ace Of Spades\",\"3\":\"106\"},{\"1\":\"161\",\"2\":\"Demorou...\",\"3\":\"108\"},{\"1\":\"162\",\"2\":\"Motley Crue Greatest Hits\",\"3\":\"109\"},{\"1\":\"163\",\"2\":\"From The Muddy Banks Of The Wishkah [Live]\",\"3\":\"110\"},{\"1\":\"164\",\"2\":\"Nevermind\",\"3\":\"110\"},{\"1\":\"165\",\"2\":\"Compositores\",\"3\":\"111\"},{\"1\":\"166\",\"2\":\"Olodum\",\"3\":\"112\"},{\"1\":\"167\",\"2\":\"Acústico MTV\",\"3\":\"113\"},{\"1\":\"168\",\"2\":\"Arquivo II\",\"3\":\"113\"},{\"1\":\"169\",\"2\":\"Arquivo Os Paralamas Do Sucesso\",\"3\":\"113\"},{\"1\":\"170\",\"2\":\"Bark at the Moon (Remastered)\",\"3\":\"114\"},{\"1\":\"171\",\"2\":\"Blizzard of Ozz\",\"3\":\"114\"},{\"1\":\"172\",\"2\":\"Diary of a Madman (Remastered)\",\"3\":\"114\"},{\"1\":\"173\",\"2\":\"No More Tears (Remastered)\",\"3\":\"114\"},{\"1\":\"174\",\"2\":\"Tribute\",\"3\":\"114\"},{\"1\":\"175\",\"2\":\"Walking Into Clarksdale\",\"3\":\"115\"},{\"1\":\"176\",\"2\":\"Original Soundtracks 1\",\"3\":\"116\"},{\"1\":\"177\",\"2\":\"The Beast Live\",\"3\":\"117\"},{\"1\":\"178\",\"2\":\"Live On Two Legs [Live]\",\"3\":\"118\"},{\"1\":\"179\",\"2\":\"Pearl Jam\",\"3\":\"118\"},{\"1\":\"180\",\"2\":\"Riot Act\",\"3\":\"118\"},{\"1\":\"181\",\"2\":\"Ten\",\"3\":\"118\"},{\"1\":\"182\",\"2\":\"Vs.\",\"3\":\"118\"},{\"1\":\"183\",\"2\":\"Dark Side Of The Moon\",\"3\":\"120\"},{\"1\":\"184\",\"2\":\"Os Cães Ladram Mas A Caravana Não Pára\",\"3\":\"121\"},{\"1\":\"185\",\"2\":\"Greatest Hits I\",\"3\":\"51\"},{\"1\":\"186\",\"2\":\"News Of The World\",\"3\":\"51\"},{\"1\":\"187\",\"2\":\"Out Of Time\",\"3\":\"122\"},{\"1\":\"188\",\"2\":\"Green\",\"3\":\"124\"},{\"1\":\"189\",\"2\":\"New Adventures In Hi-Fi\",\"3\":\"124\"},{\"1\":\"190\",\"2\":\"The Best Of R.E.M.: The IRS Years\",\"3\":\"124\"},{\"1\":\"191\",\"2\":\"Cesta Básica\",\"3\":\"125\"},{\"1\":\"192\",\"2\":\"Raul Seixas\",\"3\":\"126\"},{\"1\":\"193\",\"2\":\"Blood Sugar Sex Magik\",\"3\":\"127\"},{\"1\":\"194\",\"2\":\"By The Way\",\"3\":\"127\"},{\"1\":\"195\",\"2\":\"Californication\",\"3\":\"127\"},{\"1\":\"196\",\"2\":\"Retrospective I (1974-1980)\",\"3\":\"128\"},{\"1\":\"197\",\"2\":\"Santana - As Years Go By\",\"3\":\"59\"},{\"1\":\"198\",\"2\":\"Santana Live\",\"3\":\"59\"},{\"1\":\"199\",\"2\":\"Maquinarama\",\"3\":\"130\"},{\"1\":\"200\",\"2\":\"O Samba Poconé\",\"3\":\"130\"},{\"1\":\"201\",\"2\":\"Judas 0: B-Sides and Rarities\",\"3\":\"131\"},{\"1\":\"202\",\"2\":\"Rotten Apples: Greatest Hits\",\"3\":\"131\"},{\"1\":\"203\",\"2\":\"A-Sides\",\"3\":\"132\"},{\"1\":\"204\",\"2\":\"Morning Dance\",\"3\":\"53\"},{\"1\":\"205\",\"2\":\"In Step\",\"3\":\"133\"},{\"1\":\"206\",\"2\":\"Core\",\"3\":\"134\"},{\"1\":\"207\",\"2\":\"Mezmerize\",\"3\":\"135\"},{\"1\":\"208\",\"2\":\"[1997] Black Light Syndrome\",\"3\":\"136\"},{\"1\":\"209\",\"2\":\"Live [Disc 1]\",\"3\":\"137\"},{\"1\":\"210\",\"2\":\"Live [Disc 2]\",\"3\":\"137\"},{\"1\":\"211\",\"2\":\"The Singles\",\"3\":\"138\"},{\"1\":\"212\",\"2\":\"Beyond Good And Evil\",\"3\":\"139\"},{\"1\":\"213\",\"2\":\"Pure Cult: The Best Of The Cult (For Rockers, Ravers, Lovers & Sinners) [UK]\",\"3\":\"139\"},{\"1\":\"214\",\"2\":\"The Doors\",\"3\":\"140\"},{\"1\":\"215\",\"2\":\"The Police Greatest Hits\",\"3\":\"141\"},{\"1\":\"216\",\"2\":\"Hot Rocks, 1964-1971 (Disc 1)\",\"3\":\"142\"},{\"1\":\"217\",\"2\":\"No Security\",\"3\":\"142\"},{\"1\":\"218\",\"2\":\"Voodoo Lounge\",\"3\":\"142\"},{\"1\":\"219\",\"2\":\"Tangents\",\"3\":\"143\"},{\"1\":\"220\",\"2\":\"Transmission\",\"3\":\"143\"},{\"1\":\"221\",\"2\":\"My Generation - The Very Best Of The Who\",\"3\":\"144\"},{\"1\":\"222\",\"2\":\"Serie Sem Limite (Disc 1)\",\"3\":\"145\"},{\"1\":\"223\",\"2\":\"Serie Sem Limite (Disc 2)\",\"3\":\"145\"},{\"1\":\"224\",\"2\":\"Acústico\",\"3\":\"146\"},{\"1\":\"225\",\"2\":\"Volume Dois\",\"3\":\"146\"},{\"1\":\"226\",\"2\":\"Battlestar Galactica: The Story So Far\",\"3\":\"147\"},{\"1\":\"227\",\"2\":\"Battlestar Galactica, Season 3\",\"3\":\"147\"},{\"1\":\"228\",\"2\":\"Heroes, Season 1\",\"3\":\"148\"},{\"1\":\"229\",\"2\":\"Lost, Season 3\",\"3\":\"149\"},{\"1\":\"230\",\"2\":\"Lost, Season 1\",\"3\":\"149\"},{\"1\":\"231\",\"2\":\"Lost, Season 2\",\"3\":\"149\"},{\"1\":\"232\",\"2\":\"Achtung Baby\",\"3\":\"150\"},{\"1\":\"233\",\"2\":\"All That You Can't Leave Behind\",\"3\":\"150\"},{\"1\":\"234\",\"2\":\"B-Sides 1980-1990\",\"3\":\"150\"},{\"1\":\"235\",\"2\":\"How To Dismantle An Atomic Bomb\",\"3\":\"150\"},{\"1\":\"236\",\"2\":\"Pop\",\"3\":\"150\"},{\"1\":\"237\",\"2\":\"Rattle And Hum\",\"3\":\"150\"},{\"1\":\"238\",\"2\":\"The Best Of 1980-1990\",\"3\":\"150\"},{\"1\":\"239\",\"2\":\"War\",\"3\":\"150\"},{\"1\":\"240\",\"2\":\"Zooropa\",\"3\":\"150\"},{\"1\":\"241\",\"2\":\"UB40 The Best Of - Volume Two [UK]\",\"3\":\"151\"},{\"1\":\"242\",\"2\":\"Diver Down\",\"3\":\"152\"},{\"1\":\"243\",\"2\":\"The Best Of Van Halen, Vol. I\",\"3\":\"152\"},{\"1\":\"244\",\"2\":\"Van Halen\",\"3\":\"152\"},{\"1\":\"245\",\"2\":\"Van Halen III\",\"3\":\"152\"},{\"1\":\"246\",\"2\":\"Contraband\",\"3\":\"153\"},{\"1\":\"247\",\"2\":\"Vinicius De Moraes\",\"3\":\"72\"},{\"1\":\"248\",\"2\":\"Ao Vivo [IMPORT]\",\"3\":\"155\"},{\"1\":\"249\",\"2\":\"The Office, Season 1\",\"3\":\"156\"},{\"1\":\"250\",\"2\":\"The Office, Season 2\",\"3\":\"156\"},{\"1\":\"251\",\"2\":\"The Office, Season 3\",\"3\":\"156\"},{\"1\":\"252\",\"2\":\"Un-Led-Ed\",\"3\":\"157\"},{\"1\":\"253\",\"2\":\"Battlestar Galactica (Classic), Season 1\",\"3\":\"158\"},{\"1\":\"254\",\"2\":\"Aquaman\",\"3\":\"159\"},{\"1\":\"255\",\"2\":\"Instant Karma: The Amnesty International Campaign to Save Darfur\",\"3\":\"150\"},{\"1\":\"256\",\"2\":\"Speak of the Devil\",\"3\":\"114\"},{\"1\":\"257\",\"2\":\"20th Century Masters - The Millennium Collection: The Best of Scorpions\",\"3\":\"179\"},{\"1\":\"258\",\"2\":\"House of Pain\",\"3\":\"180\"},{\"1\":\"259\",\"2\":\"Radio Brasil (O Som da Jovem Vanguarda) - Seleccao de Henrique Amaro\",\"3\":\"36\"},{\"1\":\"260\",\"2\":\"Cake: B-Sides and Rarities\",\"3\":\"196\"},{\"1\":\"261\",\"2\":\"LOST, Season 4\",\"3\":\"149\"},{\"1\":\"262\",\"2\":\"Quiet Songs\",\"3\":\"197\"},{\"1\":\"263\",\"2\":\"Muso Ko\",\"3\":\"198\"},{\"1\":\"264\",\"2\":\"Realize\",\"3\":\"199\"},{\"1\":\"265\",\"2\":\"Every Kind of Light\",\"3\":\"200\"},{\"1\":\"266\",\"2\":\"Duos II\",\"3\":\"201\"},{\"1\":\"267\",\"2\":\"Worlds\",\"3\":\"202\"},{\"1\":\"268\",\"2\":\"The Best of Beethoven\",\"3\":\"203\"},{\"1\":\"269\",\"2\":\"Temple of the Dog\",\"3\":\"204\"},{\"1\":\"270\",\"2\":\"Carry On\",\"3\":\"205\"},{\"1\":\"271\",\"2\":\"Revelations\",\"3\":\"8\"},{\"1\":\"272\",\"2\":\"Adorate Deum: Gregorian Chant from the Proper of the Mass\",\"3\":\"206\"},{\"1\":\"273\",\"2\":\"Allegri: Miserere\",\"3\":\"207\"},{\"1\":\"274\",\"2\":\"Pachelbel: Canon & Gigue\",\"3\":\"208\"},{\"1\":\"275\",\"2\":\"Vivaldi: The Four Seasons\",\"3\":\"209\"},{\"1\":\"276\",\"2\":\"Bach: Violin Concertos\",\"3\":\"210\"},{\"1\":\"277\",\"2\":\"Bach: Goldberg Variations\",\"3\":\"211\"},{\"1\":\"278\",\"2\":\"Bach: The Cello Suites\",\"3\":\"212\"},{\"1\":\"279\",\"2\":\"Handel: The Messiah (Highlights)\",\"3\":\"213\"},{\"1\":\"280\",\"2\":\"The World of Classical Favourites\",\"3\":\"214\"},{\"1\":\"281\",\"2\":\"Sir Neville Marriner: A Celebration\",\"3\":\"215\"},{\"1\":\"282\",\"2\":\"Mozart: Wind Concertos\",\"3\":\"216\"},{\"1\":\"283\",\"2\":\"Haydn: Symphonies 99 - 104\",\"3\":\"217\"},{\"1\":\"284\",\"2\":\"Beethoven: Symhonies Nos. 5 & 6\",\"3\":\"218\"},{\"1\":\"285\",\"2\":\"A Soprano Inspired\",\"3\":\"219\"},{\"1\":\"286\",\"2\":\"Great Opera Choruses\",\"3\":\"220\"},{\"1\":\"287\",\"2\":\"Wagner: Favourite Overtures\",\"3\":\"221\"},{\"1\":\"288\",\"2\":\"Fauré: Requiem, Ravel: Pavane & Others\",\"3\":\"222\"},{\"1\":\"289\",\"2\":\"Tchaikovsky: The Nutcracker\",\"3\":\"223\"},{\"1\":\"290\",\"2\":\"The Last Night of the Proms\",\"3\":\"224\"},{\"1\":\"291\",\"2\":\"Puccini: Madama Butterfly - Highlights\",\"3\":\"225\"},{\"1\":\"292\",\"2\":\"Holst: The Planets, Op. 32 & Vaughan Williams: Fantasies\",\"3\":\"226\"},{\"1\":\"293\",\"2\":\"Pavarotti's Opera Made Easy\",\"3\":\"227\"},{\"1\":\"294\",\"2\":\"Great Performances - Barber's Adagio and Other Romantic Favorites for Strings\",\"3\":\"228\"},{\"1\":\"295\",\"2\":\"Carmina Burana\",\"3\":\"229\"},{\"1\":\"296\",\"2\":\"A Copland Celebration, Vol. I\",\"3\":\"230\"},{\"1\":\"297\",\"2\":\"Bach: Toccata & Fugue in D Minor\",\"3\":\"231\"},{\"1\":\"298\",\"2\":\"Prokofiev: Symphony No.1\",\"3\":\"232\"},{\"1\":\"299\",\"2\":\"Scheherazade\",\"3\":\"233\"},{\"1\":\"300\",\"2\":\"Bach: The Brandenburg Concertos\",\"3\":\"234\"},{\"1\":\"301\",\"2\":\"Chopin: Piano Concertos Nos. 1 & 2\",\"3\":\"235\"},{\"1\":\"302\",\"2\":\"Mascagni: Cavalleria Rusticana\",\"3\":\"236\"},{\"1\":\"303\",\"2\":\"Sibelius: Finlandia\",\"3\":\"237\"},{\"1\":\"304\",\"2\":\"Beethoven Piano Sonatas: Moonlight & Pastorale\",\"3\":\"238\"},{\"1\":\"305\",\"2\":\"Great Recordings of the Century - Mahler: Das Lied von der Erde\",\"3\":\"240\"},{\"1\":\"306\",\"2\":\"Elgar: Cello Concerto & Vaughan Williams: Fantasias\",\"3\":\"241\"},{\"1\":\"307\",\"2\":\"Adams, John: The Chairman Dances\",\"3\":\"242\"},{\"1\":\"308\",\"2\":\"Tchaikovsky: 1812 Festival Overture, Op.49, Capriccio Italien & Beethoven: Wellington's Victory\",\"3\":\"243\"},{\"1\":\"309\",\"2\":\"Palestrina: Missa Papae Marcelli & Allegri: Miserere\",\"3\":\"244\"},{\"1\":\"310\",\"2\":\"Prokofiev: Romeo & Juliet\",\"3\":\"245\"},{\"1\":\"311\",\"2\":\"Strauss: Waltzes\",\"3\":\"226\"},{\"1\":\"312\",\"2\":\"Berlioz: Symphonie Fantastique\",\"3\":\"245\"},{\"1\":\"313\",\"2\":\"Bizet: Carmen Highlights\",\"3\":\"246\"},{\"1\":\"314\",\"2\":\"English Renaissance\",\"3\":\"247\"},{\"1\":\"315\",\"2\":\"Handel: Music for the Royal Fireworks (Original Version 1749)\",\"3\":\"208\"},{\"1\":\"316\",\"2\":\"Grieg: Peer Gynt Suites & Sibelius: Pelléas et Mélisande\",\"3\":\"248\"},{\"1\":\"317\",\"2\":\"Mozart Gala: Famous Arias\",\"3\":\"249\"},{\"1\":\"318\",\"2\":\"SCRIABIN: Vers la flamme\",\"3\":\"250\"},{\"1\":\"319\",\"2\":\"Armada: Music from the Courts of England and Spain\",\"3\":\"251\"},{\"1\":\"320\",\"2\":\"Mozart: Symphonies Nos. 40 & 41\",\"3\":\"248\"},{\"1\":\"321\",\"2\":\"Back to Black\",\"3\":\"252\"},{\"1\":\"322\",\"2\":\"Frank\",\"3\":\"252\"},{\"1\":\"323\",\"2\":\"Carried to Dust (Bonus Track Version)\",\"3\":\"253\"},{\"1\":\"324\",\"2\":\"Beethoven: Symphony No. 6 'Pastoral' Etc.\",\"3\":\"254\"},{\"1\":\"325\",\"2\":\"Bartok: Violin & Viola Concertos\",\"3\":\"255\"},{\"1\":\"326\",\"2\":\"Mendelssohn: A Midsummer Night's Dream\",\"3\":\"256\"},{\"1\":\"327\",\"2\":\"Bach: Orchestral Suites Nos. 1 - 4\",\"3\":\"257\"},{\"1\":\"328\",\"2\":\"Charpentier: Divertissements, Airs & Concerts\",\"3\":\"258\"},{\"1\":\"329\",\"2\":\"South American Getaway\",\"3\":\"259\"},{\"1\":\"330\",\"2\":\"Górecki: Symphony No. 3\",\"3\":\"260\"},{\"1\":\"331\",\"2\":\"Purcell: The Fairy Queen\",\"3\":\"261\"},{\"1\":\"332\",\"2\":\"The Ultimate Relexation Album\",\"3\":\"262\"},{\"1\":\"333\",\"2\":\"Purcell: Music for the Queen Mary\",\"3\":\"263\"},{\"1\":\"334\",\"2\":\"Weill: The Seven Deadly Sins\",\"3\":\"264\"},{\"1\":\"335\",\"2\":\"J.S. Bach: Chaconne, Suite in E Minor, Partita in E Major & Prelude, Fugue and Allegro\",\"3\":\"265\"},{\"1\":\"336\",\"2\":\"Prokofiev: Symphony No.5 & Stravinksy: Le Sacre Du Printemps\",\"3\":\"248\"},{\"1\":\"337\",\"2\":\"Szymanowski: Piano Works, Vol. 1\",\"3\":\"266\"},{\"1\":\"338\",\"2\":\"Nielsen: The Six Symphonies\",\"3\":\"267\"},{\"1\":\"339\",\"2\":\"Great Recordings of the Century: Paganini's 24 Caprices\",\"3\":\"268\"},{\"1\":\"340\",\"2\":\"Liszt - 12 Études D'Execution Transcendante\",\"3\":\"269\"},{\"1\":\"341\",\"2\":\"Great Recordings of the Century - Shubert: Schwanengesang, 4 Lieder\",\"3\":\"270\"},{\"1\":\"342\",\"2\":\"Locatelli: Concertos for Violin, Strings and Continuo, Vol. 3\",\"3\":\"271\"},{\"1\":\"343\",\"2\":\"Respighi:Pines of Rome\",\"3\":\"226\"},{\"1\":\"344\",\"2\":\"Schubert: The Late String Quartets & String Quintet (3 CD's)\",\"3\":\"272\"},{\"1\":\"345\",\"2\":\"Monteverdi: L'Orfeo\",\"3\":\"273\"},{\"1\":\"346\",\"2\":\"Mozart: Chamber Music\",\"3\":\"274\"},{\"1\":\"347\",\"2\":\"Koyaanisqatsi (Soundtrack from the Motion Picture)\",\"3\":\"275\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Save data to memory\nalbum_tbl <- tbl(con, \"Album\") %>% collect()\n\n# Close database connection\ndbDisconnect(con)\ncon\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> <SQLiteConnection>\n#>   DISCONNECTED\n```\n:::\n\n```{.r .cell-code}\n# glue string interpolation example\nname <- \"Fred\"\nglue('My name is {name}.')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> My name is Fred.\n```\n:::\n\n```{.r .cell-code}\n# API GET example\nresp <- GET(\"https://swapi.dev/api/people/1/\")\n\n# Wrapped into a function\nsw_api <- function(path) {\n  url <- modify_url(url = \"https://swapi.dev\", path = glue(\"/api{path}\"))\n  resp <- GET(url)\n  stop_for_status(resp) # automatically throws an error if a request did not succeed\n}\n\n# API GET function example\nresp <- sw_api(\"/people/1\")\nresp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Response [https://swapi.dev/api/people/1]\n#>   Date: 2023-05-22 21:13\n#>   Status: 200\n#>   Content-Type: application/json\n#>   Size: 647 B\n```\n:::\n\n```{.r .cell-code}\n# convert API response body\nresp_extracted <- rawToChar(resp$content)  %>% fromJSON()\nresp_extracted\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> $name\n#> [1] \"Luke Skywalker\"\n#> \n#> $height\n#> [1] \"172\"\n#> \n#> $mass\n#> [1] \"77\"\n#> \n#> $hair_color\n#> [1] \"blond\"\n#> \n#> $skin_color\n#> [1] \"fair\"\n#> \n#> $eye_color\n#> [1] \"blue\"\n#> \n#> $birth_year\n#> [1] \"19BBY\"\n#> \n#> $gender\n#> [1] \"male\"\n#> \n#> $homeworld\n#> [1] \"https://swapi.dev/api/planets/1/\"\n#> \n#> $films\n#> [1] \"https://swapi.dev/api/films/1/\" \"https://swapi.dev/api/films/2/\"\n#> [3] \"https://swapi.dev/api/films/3/\" \"https://swapi.dev/api/films/6/\"\n#> \n#> $species\n#> list()\n#> \n#> $vehicles\n#> [1] \"https://swapi.dev/api/vehicles/14/\" \"https://swapi.dev/api/vehicles/30/\"\n#> \n#> $starships\n#> [1] \"https://swapi.dev/api/starships/12/\" \"https://swapi.dev/api/starships/22/\"\n#> \n#> $created\n#> [1] \"2014-12-09T13:50:51.644000Z\"\n#> \n#> $edited\n#> [1] \"2014-12-20T21:17:56.891000Z\"\n#> \n#> $url\n#> [1] \"https://swapi.dev/api/people/1/\"\n```\n:::\n\n```{.r .cell-code}\n# Test list\ndata_list <- list(strings= c(\"string1\", \"string2\"), \n                  numbers = c(1,2,3), \n                  TRUE, \n                  100.23, \n                  tibble(\n                    A = c(1,2), \n                    B = c(\"x\", \"y\")\n                  )\n)\n\n# access list\nresp %>% \n  .$content %>% \n  rawToChar() %>% \n  fromJSON()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> $name\n#> [1] \"Luke Skywalker\"\n#> \n#> $height\n#> [1] \"172\"\n#> \n#> $mass\n#> [1] \"77\"\n#> \n#> $hair_color\n#> [1] \"blond\"\n#> \n#> $skin_color\n#> [1] \"fair\"\n#> \n#> $eye_color\n#> [1] \"blue\"\n#> \n#> $birth_year\n#> [1] \"19BBY\"\n#> \n#> $gender\n#> [1] \"male\"\n#> \n#> $homeworld\n#> [1] \"https://swapi.dev/api/planets/1/\"\n#> \n#> $films\n#> [1] \"https://swapi.dev/api/films/1/\" \"https://swapi.dev/api/films/2/\"\n#> [3] \"https://swapi.dev/api/films/3/\" \"https://swapi.dev/api/films/6/\"\n#> \n#> $species\n#> list()\n#> \n#> $vehicles\n#> [1] \"https://swapi.dev/api/vehicles/14/\" \"https://swapi.dev/api/vehicles/30/\"\n#> \n#> $starships\n#> [1] \"https://swapi.dev/api/starships/12/\" \"https://swapi.dev/api/starships/22/\"\n#> \n#> $created\n#> [1] \"2014-12-09T13:50:51.644000Z\"\n#> \n#> $edited\n#> [1] \"2014-12-20T21:17:56.891000Z\"\n#> \n#> $url\n#> [1] \"https://swapi.dev/api/people/1/\"\n```\n:::\n\n```{.r .cell-code}\n# directly parse body\n# content(resp, as = \"text\")\n# content(resp, as = \"parsed\")\ncontent(resp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> $name\n#> [1] \"Luke Skywalker\"\n#> \n#> $height\n#> [1] \"172\"\n#> \n#> $mass\n#> [1] \"77\"\n#> \n#> $hair_color\n#> [1] \"blond\"\n#> \n#> $skin_color\n#> [1] \"fair\"\n#> \n#> $eye_color\n#> [1] \"blue\"\n#> \n#> $birth_year\n#> [1] \"19BBY\"\n#> \n#> $gender\n#> [1] \"male\"\n#> \n#> $homeworld\n#> [1] \"https://swapi.dev/api/planets/1/\"\n#> \n#> $films\n#> $films[[1]]\n#> [1] \"https://swapi.dev/api/films/1/\"\n#> \n#> $films[[2]]\n#> [1] \"https://swapi.dev/api/films/2/\"\n#> \n#> $films[[3]]\n#> [1] \"https://swapi.dev/api/films/3/\"\n#> \n#> $films[[4]]\n#> [1] \"https://swapi.dev/api/films/6/\"\n#> \n#> \n#> $species\n#> list()\n#> \n#> $vehicles\n#> $vehicles[[1]]\n#> [1] \"https://swapi.dev/api/vehicles/14/\"\n#> \n#> $vehicles[[2]]\n#> [1] \"https://swapi.dev/api/vehicles/30/\"\n#> \n#> \n#> $starships\n#> $starships[[1]]\n#> [1] \"https://swapi.dev/api/starships/12/\"\n#> \n#> $starships[[2]]\n#> [1] \"https://swapi.dev/api/starships/22/\"\n#> \n#> \n#> $created\n#> [1] \"2014-12-09T13:50:51.644000Z\"\n#> \n#> $edited\n#> [1] \"2014-12-20T21:17:56.891000Z\"\n#> \n#> $url\n#> [1] \"https://swapi.dev/api/people/1/\"\n```\n:::\n\n```{.r .cell-code}\n# call to alpha vantage api\nresp <- GET('https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol=WDI.DE')\nresp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Response [https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol=WDI.DE]\n#>   Date: 2023-05-22 21:13\n#>   Status: 200\n#>   Content-Type: application/json\n#>   Size: 189 B\n#> {\n#>     \"Error Message\": \"the parameter apikey is invalid or missing. Please clai...\n```\n:::\n\n```{.r .cell-code}\n# test the renviron file\nuserid <- Sys.getenv('userid')\npwd <- Sys.getenv('pwd')\nkey <- Sys.getenv('key')\n\n# web scraping example\n# get the URL for the wikipedia page with all S&P 500 symbols\nurl <- \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n# use that URL to scrape the S&P 500 table using rvest\nlibrary(rvest)\nsp_500 <- url %>%\n  # read the HTML from the webpage\n  read_html() %>%\n  # Get the nodes with the id\n  html_nodes(css = \"#constituents\") %>%\n  # html_nodes(xpath = \"//*[@id='constituents']\"\") %>% \n  # Extract the table and turn the list into a tibble\n  html_table() %>% \n  .[[1]] %>% \n  as_tibble()\n\n# web scraping example 2\nurl  <- \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\"\nhtml <- url %>% \n  read_html\n\n# get the ranks\nrank <-  html %>% \n  html_nodes(css = \".titleColumn\") %>% \n  html_text() %>% \n  # Extrag all digits between \" \" and \".\\n\" The \"\\\" have to be escaped\n  # You can use Look ahead \"<=\" and Look behind \"?=\" for this\n  stringr::str_extract(\"(?<= )[0-9]*(?=\\\\.\\\\n)\")%>% \n  # Make all values numeric\n  as.numeric()\n\n# get the titles\ntitle <- html %>% \n  html_nodes(\".titleColumn > a\") %>% \n  html_text()\n\n# get the year\nyear <- html %>% \n  html_nodes(\".titleColumn .secondaryInfo\") %>%\n  html_text() %>% \n  # Extract numbers\n  stringr::str_extract(pattern = \"[0-9]+\") %>% \n  as.numeric()\n\n# get the people\npeople <- html %>% \n  html_nodes(\".titleColumn > a\") %>% \n  html_attr(\"title\")\n\n# get the ratings\nrating <- html %>% \n  html_nodes(css = \".imdbRating > strong\") %>% \n  html_text() %>% \n  as.numeric()\n\n# get the number of ratings\nnum_ratings <- html %>% \n  html_nodes(css = \".imdbRating > strong\") %>% \n  html_attr('title') %>% \n  # Extract the numbers and remove the comma to make it numeric values\n  stringr::str_extract(\"(?<=based on ).*(?=\\ user ratings)\" ) %>% \n  stringr::str_replace_all(pattern = \",\", replacement = \"\") %>% \n  as.numeric()\n\n# merge everything\nimdb_tbl <- tibble(rank, title, year, people, rating, num_ratings)\n\n# # read bike data from json file\n# bike_data_lst <- fromJSON(\"bike_data.json\")\n# # Open the data by clicking on it in the environment or by running View()\n# View(bike_data_lst)\n# \n# # color path\n# # productDetail --> variationAttributes --> values --> [[1]] --> displayValue\n# bike_data_lst[[\"productDetail\"]][[\"variationAttributes\"]][[\"values\"]][[1]][[\"displayValue\"]]\n# \n# # color extraction using pluck\n# bike_data_lst %>%\n#   purrr::pluck(\"productDetail\", \"variationAttributes\", \"values\", 1, \"displayValue\")\n\n# Scraping example\n  # # 1.1 collect product families ----\n  # \n  # url_home          <- \"https://www.canyon.com/en-de\"\n  # xopen(url_home) # Open links directly from RStudio to inspect them\n  # \n  # # Read in the HTML for the entire webpage\n  # html_home         <- read_html(url_home)\n  # \n  # # Web scrape the ids for the families\n  # bike_family_tbl <- html_home %>%\n  #   \n  #   # Get the nodes for the families ...\n  #   html_nodes(css = \".js-navigationDrawer__list--secondary\") %>%\n  #   # ...and extract the information of the id attribute\n  #   html_attr('id') %>%\n  #   \n  #   # Remove the product families Gear and Outlet and Woman \n  #   # (because the female bikes are also listed with the others)\n  #   discard(.p = ~stringr::str_detect(.x,\"WMN|WOMEN|GEAR|OUTLET\")) %>%\n  #   \n  #   # Convert vector to tibble\n  #   enframe(name = \"position\", value = \"family_class\") %>%\n  #   \n  #   # Add a hashtag so we can get nodes of the categories by id (#)\n  #   mutate(\n  #     family_id = str_glue(\"#{family_class}\")\n  #   )\n  # \n  # bike_family_tbl\n  # \n  # # 1.2 COLLECT PRODUCT CATEGORIES ----\n  # \n  # # Combine all Ids to one string so that we will get all nodes at once\n  # # (seperated by the OR operator \",\")\n  # family_id_css <- bike_family_tbl %>%\n  #   pull(family_id) %>%\n  #   stringr::str_c(collapse = \", \")\n  # family_id_css\n  # ## \"#js-navigationList-ROAD, #js-navigationList-MOUNTAIN, #js-navigationList-EBIKES, #js-navigationList-HYBRID-CITY, #js-navigationList-YOUNGHEROES\"\n  # \n  # # Extract the urls from the href attribute\n  # bike_category_tbl <- html_home %>%\n  #   \n  #   # Select nodes by the ids\n  #   html_nodes(css = family_id_css) %>%\n  #   \n  #   # Going further down the tree and select nodes by class\n  #   # Selecting two classes makes it specific enough\n  #   html_nodes(css = \".navigationListSecondary__listItem .js-ridestyles\") %>%\n  #   html_attr('href') %>%\n  #   \n  #   # Convert vector to tibble\n  #   enframe(name = \"position\", value = \"subdirectory\") %>%\n  #   \n  #   # Add the domain, because we will get only the subdirectories\n  #   mutate(\n  #     url = glue(\"https://www.canyon.com{subdirectory}\")\n  #   ) %>%\n  #   \n  #   # Some categories are listed multiple times.\n  #   # We only need unique values\n  #   distinct(url)\n  # \n  # bike_category_tbl\n  # \n  # # 2.0 COLLECT BIKE DATA ----\n  # \n  # # 2.1 Get URL for each bike of the Product categories\n  # \n  # # select first bike category url\n  # bike_category_url <- bike_category_tbl$url[1]\n  # \n  # # Alternatives for selecting values\n  # # bike_category_url <- bike_category_tbl %$% url %>% .[1]\n  # # bike_category_url <- bike_category_tbl %>% pull(url) %>% .[1]\n  # # bike_category_url <- deframe(bike_category_tbl[1,])\n  # # bike_category_url <- bike_category_tbl %>% first %>% first\n  # \n  # xopen(bike_category_url)\n  # \n  # # Get the URLs for the bikes of the first category\n  # html_bike_category  <- read_html(bike_category_url)\n  # bike_url_tbl        <- html_bike_category %>%\n  #   \n  #   # Get the 'a' nodes, which are hierarchally underneath \n  #   # the class productTile__contentWrapper\n  #   html_nodes(css = \".productTile__contentWrapper > a\") %>%\n  #   html_attr(\"href\") %>%\n  #   \n  #   # Remove the query parameters of the URL (everything after the '?')\n  #   str_remove(pattern = \"\\\\?.*\") %>%\n  #   \n  #   # Convert vector to tibble\n  #   enframe(name = \"position\", value = \"url\")\n  # \n  # # 2.1.2 Extract the descriptions (since we have retrieved the data already)\n  # bike_desc_tbl <- html_bike_category %>%\n  #   \n  #   # Get the nodes in the meta tag where the attribute itemprop equals description\n  #   html_nodes('.productTile__productSummaryLeft > meta[itemprop=\"description\"]') %>%\n  #   \n  #   # Extract the content of the attribute content\n  #   html_attr(\"content\") %>%\n  #   \n  #   # Convert vector to tibble\n  #   enframe(name = \"position\", value = \"description\")\n  # \n  # # 2.1.3 Get even more data from JSON files\n  # bike_json_tbl  <- html_bike_category %>%\n  #   \n  #   html_nodes(css = '.productGrid__listItem.xlt-producttile > div') %>%\n  #   html_attr(\"data-gtm-impression\") %>%\n  #   \n  #   # Convert the JSON format to dataframe\n  #   # map runs that function on each element of the list\n  #   map(fromJSON) %>% # need JSON ### need lists\n  #   \n  #   # Extract relevant information of the nested list\n  #   map(purrr::pluck, 2, \"impressions\") %>% # Need purrr and expl above\n  #   \n  #   # Set \"not defined\" and emtpy fields to NA (will be easier to work with)\n  #   map(na_if, \"not defined\") %>%\n  #   map(na_if, \"\") %>%\n  #   \n  #   # The class of dimension56 and price varies between numeric and char.\n  #   # This converts this column in each list to numeric\n  #   # across allows to perform the same operation on multiple columns\n  #   map(~mutate(., across(c(\"dimension56\",\"price\"), as.numeric))) %>%\n  #   \n  #   # Stack all lists together\n  #   bind_rows() %>%\n  #   # Convert to tibble so that we have the same data format\n  #   as_tibble() %>%\n  #   \n  #   # Add consecutive numbers so that we can bind all data together\n  #   # You could have also just use bind_cols()\n  #   rowid_to_column(var='position') %>%\n  #   left_join(bike_desc_tbl) %>%\n  #   left_join(bike_url_tbl)\n  # \n  # # 2.2 Wrap it into a function ----\n  # get_bike_data <- function(url) {\n  #   \n  #   html_bike_category <- read_html(url)\n  #   \n  #   # Get the URLs\n  #   bike_url_tbl  <- html_bike_category %>%\n  #     html_nodes(css = \".productTile__contentWrapper > a\") %>%\n  #     html_attr(\"href\") %>%\n  #     str_remove(pattern = \"\\\\?.*\") %>%\n  #     enframe(name = \"position\", value = \"url\")\n  #   \n  #   # Get the descriptions\n  #   bike_desc_tbl <- html_bike_category %>%\n  #     html_nodes(css = '.productTile__productSummaryLeft > \n  #                       meta[itemprop=\"description\"]') %>%\n  #     html_attr(\"content\") %>%\n  #     enframe(name = \"position\", value = \"description\")\n  #   \n  #   # Get JSON data\n  #   bike_json_tbl <- html_bike_category %>%\n  #     html_nodes(css = '.productGrid__listItem.xlt-producttile > div') %>%\n  #     html_attr(\"data-gtm-impression\") %>%\n  #     map(fromJSON) %>% # need JSON ### need lists\n  #     map(purrr::pluck, 2, \"impressions\") %>% \n  #     map(na_if, \"not defined\") %>%\n  #     map(na_if, \"\") %>%\n  #     map(~mutate(., across(c(\"dimension56\",\"price\"), as.numeric))) %>%\n  #     bind_rows() %>%\n  #     as_tibble() %>%\n  #     rowid_to_column(var='position') %>%\n  #     left_join(bike_desc_tbl) %>%\n  #     left_join(bike_url_tbl)\n  # }\n  # \n  # # Run the function with the first url to check if it is working\n  # bike_category_url <- bike_category_tbl$url[1]\n  # bike_data_tbl     <- get_bike_data(url = bike_category_url)\n  # \n  # bike_data_tbl\n  # \n  # # 2.3.1a Map the function against all urls\n  # \n  # # Extract the urls as a character vector\n  # bike_category_url_vec <- bike_category_tbl %>% \n  #   pull(url)\n  # \n  # # Run the function with every url as an argument\n  # bike_data_lst <- map(bike_category_url_vec, get_bike_data)\n  # \n  # # Merge the list into a tibble\n  # bike_data_tbl <- bind_rows(bike_data_lst)\n  # saveRDS(bike_data_tbl, \"bike_data_tbl.rds\")\n  # \n  # # 2.3.1b Alternative with a for loop\n  # \n  # # Create an empty tibble, that we can populate\n  # bike_data_tbl <- tibble()\n  # \n  # # Loop through all urls\n  # for (i in seq_along(bike_category_tbl$url)) {\n  #   \n  #   bike_category_url <- bike_category_tbl$url[i]\n  #   bike_data_tbl     <- bind_rows(bike_data_tbl, get_bike_data(bike_category_url))\n  #   \n  #   # Wait between each request to reduce the load on the server \n  #   # Otherwise we could get blocked\n  #   Sys.sleep(5)\n  #   \n  #   # print the progress\n  #   print(i)\n  #   \n  # }\n  # \n  # # Check for duplicates\n  # bike_data_tbl %>%\n  #   group_by(id) %>%\n  #   filter(n()>1) %>%\n  #   arrange(id) %>% \n  #   View()\n  # \n  # # Filter non Canyon bikes (based on id length) and add an empty column for the colors\n  # bike_data_cleaned_tbl <- bike_data_tbl %>%\n  #   \n  #   # Filter for bikes. Only unique ones\n  #   filter(nchar(.$id) == 4) %>%\n  #   filter(!(name %>% str_detect(\"Frameset\"))) %>%\n  #   distinct(id, .keep_all = T) %>%\n  #   \n  #   # Split categories (Speedmax had to be treated individually)\n  #   mutate(category = replace(category, \n  #                             name == \"Speedmax CF SLX 8.0 SL\", \"Road/Triathlon Bike/Speedmax\")) %>%\n  #   separate(col = category, into = c(\"category_1\",\n  #                                     \"category_2\",\n  #                                     \"category_3\"),\n  #            sep = \"(?<!\\\\s)/(?!\\\\s)\") %>%\n  #   \n  #   # Renaming\n  #   rename(\"year\"       = \"dimension50\") %>%\n  #   rename(\"model\"      = \"name\") %>%\n  #   rename(\"gender\"     = \"dimension63\") %>%\n  #   rename(\"price_euro\" = \"metric4\") %>%\n  #   \n  #   # Fix years manually (have checked the website)\n  #   mutate(year = replace_na(year, 2021)) %>%\n  #   \n  #   # Add frame material\n  #   mutate(frame_material = case_when(\n  #     model %>% str_detect(\" CF \") ~ \"carbon\",\n  #     model %>% str_detect(\" CFR \") ~ \"carbon\",\n  #     TRUE ~ \"aluminium\"\n  #   )\n  #   ) %>%\n  #   \n  #   # Select and order columns\n  #   select(-c(position, brand, variant, starts_with(\"dim\"), \n  #             quantity, feedProductId, price, metric5)) %>%\n  #   select(id, model, year, frame_material, price_euro, everything())\n  # \n  # saveRDS(bike_data_cleaned_tbl, \"bike_data_cleaned_tbl.rds\")\n  # \n  # # 3.1a Get all color variations for each bike\n  # \n  # # Extract all bike urls\n  # bike_url_vec <- bike_data_cleaned_tbl %>% \n  #   pull(url)\n  # \n  # # Create function to get the variations\n  # get_colors <- function(url) {\n  #   \n  #   url %>%\n  #     \n  #     read_html() %>%\n  #     \n  #     # Get all 'script nodes' and convert to char\n  #     html_nodes(css = \"script\") %>%\n  #     as.character() %>%\n  #     \n  #     # Select the node, that contains 'window.deptsfra'\n  #     str_subset(pattern = \"window.deptsfra\") %>%\n  #     \n  #     # remove the chars that do not belong to the json\n  #     # 1. replace at the beginning everything until the first \"{\" with \"\"\n  #     str_replace(\"^[^\\\\{]+\", \"\") %>%\n  #     # 2. replace at the end everything after the last \"}\" with \"\"\n  #     str_replace(\"[^\\\\}]+$\", \"\") %>%\n  #     \n  #     # Convert from json to an r object and pick the relevant values\n  #     fromJSON() %>%\n  #     purrr::pluck(\"productDetail\", \"variationAttributes\", \"values\", 1, \"value\")\n  # }\n  # \n  # # Run the function over all urls and add result to bike_data_cleaned_tbl\n  # # This will take a long time (~ 20-30 minutes) because we have to iterate over many bikes\n  # bike_data_colors_tbl <- bike_data_cleaned_tbl %>% \n  #   mutate(colors = map(bike_url_vec, get_colors))\n  # \n  # saveRDS(bike_data_colors_tbl, \"bike_data_colors_tbl.rds\")\n  # \n  # library(furrr)     # Parallel Processing using purrr (iteration)\n  # plan(\"multiprocess\")\n  # bike_data_colors_tbl <- bike_data_cleaned_tbl %>% \n  #   mutate(colors = future_map(bike_url_vec, get_colors))\n  # \n  # # 3.2 Create the urls for each variation\n  # \n  # bike_data_colors_tbl <- bike_data_colors_tbl %>%\n  #   \n  #   # Create entry for each color variation\n  #   unnest(colors) %>%\n  #   \n  #   # Merge url and query parameters for the colors\n  #   mutate(url_color = glue(\"{url}?dwvar_{id}_pv_rahmenfarbe={colors}\")) %>%\n  #   select(-url) %>%\n  #   \n  #   # Use stringi to replace the last dash with the HTLM format of a dash (%2F)\n  #   # Only if there is a dash in the color column\n  #   mutate(url_color = ifelse(str_detect(colors, pattern = \"/\"),\n  #                             \n  #                             # if TRUE --> replace      \n  #                             stringi::stri_replace_last_fixed(url_color, \"/\", \"%2F\"),\n  #                             \n  #                             # ELSE --> take the original url\n  #                             url_color))\n  # \n  # bike_data_colors_tbl %>% glimpse()\n  # \n  # # Create function\n  # get_sizes <- function(url) {\n  #   \n  #   json <- url %>%\n  #     \n  #     read_html() %>%\n  #     \n  #     # Get all 'script nodes' and convert to char\n  #     html_nodes(css = \"script\") %>%\n  #     as.character() %>%\n  #     \n  #     # Select the node, that contains 'window.deptsfra'\n  #     str_subset(pattern = \"window.deptsfra\") %>%\n  #     \n  #     # remove the chars that do not belong to the json\n  #     # 1. replace at the beginning everything until the first \"{\" with \"\"\n  #     str_replace(\"^[^\\\\{]+\", \"\") %>%\n  #     # 2. replace at the end everything after the last \"}\" with \"\"\n  #     str_replace(\"[^\\\\}]+$\", \"\") %>%\n  #     \n  #     # Convert from json to an r object and pick the relevant values\n  #     fromJSON(flatten = T) %>%\n  #     purrr::pluck(\"productDetail\", \"variationAttributes\", \"values\", 2) %>%\n  #     \n  #     # select(id, value, available, availability)# %>%\n  #     select(id, value, availability.onlyXLeftNumber) %>%\n  #     \n  #     # Rename\n  #     rename(id_size = id) %>%\n  #     rename(size = value) %>%\n  #     rename(stock_availability = availability.onlyXLeftNumber) %>%\n  #     \n  #     # Conver to tibble\n  #     as_tibble()\n  #   \n  # }\n  # \n  # # Pull url vector\n  # bike_url_color_vec <- bike_data_colors_tbl %>% \n  #   pull(url_color)\n  # \n  # # Map\n  # bike_data_sizes_tbl <- bike_data_colors_tbl %>% \n  #   mutate(size = future_map(bike_url_color_vec, get_sizes))\n  # \n  # # Unnest\n  # bike_data_sizes_tbl <- bike_data_sizes_tbl %>% \n  #   unnest(size)\n  # \n  # saveRDS(bike_data_sizes_tbl, \"bike_data_sizes_tbl.rds\")\n```\n:::\n\n</details>\n\n# Data Acquisition Challenge 1\n\nData is requested from openweathermap, some data is extracted and the result is visualized.\n\n## API call and data extraction\n\n\n\n::: {.cell hash='02_data_acquisition_cache/html/unnamed-chunk-2_0900e0d98110dcae52996381cf7b67f3'}\n\n```{.r .cell-code}\n# Challenge 2.1 ----\n\n# 1.0 LIBRARIES ----\n\nlibrary(tidyverse) # Main Package - Loads dplyr, purrr, etc.\nlibrary(rvest)     # HTML Hacking & Web Scraping\nlibrary(xopen)     # Quickly opening URLs\nlibrary(jsonlite)  # converts JSON files to R objects\nlibrary(glue)      # concatenate strings\nlibrary(stringi)   # character string/text processing\nlibrary(RSQLite)\nlibrary(httr)\n\n# Get api key from renviron file \napikey <- Sys.getenv('key')\n\n# store openweathermap city id for hamburg to variable\ncity_id <- 2911298\n\n# Call to openweathermao api \nweather <- GET(glue(\"http://api.openweathermap.org/data/2.5/forecast?id={city_id}&APPID={apikey}\"))\n\n# convert API response body\nweather_extracted <- rawToChar(weather$content)  %>% fromJSON()\nweather_extracted\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> $cod\n#> [1] \"200\"\n#> \n#> $message\n#> [1] 0\n#> \n#> $cnt\n#> [1] 40\n#> \n#> $list\n#>            dt main.temp main.feels_like main.temp_min main.temp_max\n#> 1  1684800000    288.18          288.30        285.82        288.18\n#> 2  1684810800    287.53          287.61        286.62        287.53\n#> 3  1684821600    287.54          287.57        287.54        287.54\n#> 4  1684832400    285.91          285.70        285.91        285.91\n#> 5  1684843200    287.90          287.13        287.90        287.90\n#> 6  1684854000    287.22          286.12        287.22        287.22\n#> 7  1684864800    283.66          282.57        283.66        283.66\n#> 8  1684875600    280.76          278.25        280.76        280.76\n#> 9  1684886400    280.60          278.29        280.60        280.60\n#> 10 1684897200    281.38          278.62        281.38        281.38\n#> 11 1684908000    283.49          282.93        283.49        283.49\n#> 12 1684918800    285.25          284.60        285.25        285.25\n#> 13 1684929600    286.49          285.84        286.49        286.49\n#> 14 1684940400    289.92          289.11        289.92        289.92\n#> 15 1684951200    286.59          286.21        286.59        286.59\n#> 16 1684962000    282.83          281.71        282.83        282.83\n#> 17 1684972800    281.13          279.86        281.13        281.13\n#> 18 1684983600    280.24          278.84        280.24        280.24\n#> 19 1684994400    283.32          282.43        283.32        283.32\n#> 20 1685005200    288.61          287.41        288.61        288.61\n#> 21 1685016000    291.25          290.16        291.25        291.25\n#> 22 1685026800    290.74          289.76        290.74        290.74\n#> 23 1685037600    286.36          285.43        286.36        286.36\n#> 24 1685048400    280.44          278.14        280.44        280.44\n#> 25 1685059200    278.44          276.20        278.44        278.44\n#> 26 1685070000    278.01          275.73        278.01        278.01\n#> 27 1685080800    283.15          280.80        283.15        283.15\n#> 28 1685091600    287.04          285.79        287.04        287.04\n#> 29 1685102400    289.72          288.61        289.72        289.72\n#> 30 1685113200    289.73          288.72        289.73        289.73\n#> 31 1685124000    285.84          284.97        285.84        285.84\n#> 32 1685134800    280.22          278.52        280.22        280.22\n#> 33 1685145600    278.66          277.13        278.66        278.66\n#> 34 1685156400    277.72          276.67        277.72        277.72\n#> 35 1685167200    282.84          282.84        282.84        282.84\n#> 36 1685178000    288.46          287.35        288.46        288.46\n#> 37 1685188800    291.31          290.25        291.31        291.31\n#> 38 1685199600    291.56          290.58        291.56        291.56\n#> 39 1685210400    288.30          287.70        288.30        288.30\n#> 40 1685221200    282.78          282.22        282.78        282.78\n#>    main.pressure main.sea_level main.grnd_level main.humidity main.temp_kf\n#> 1           1013           1013            1009            98         2.36\n#> 2           1012           1012            1008            99         0.91\n#> 3           1012           1012            1009            97         0.00\n#> 4           1014           1014            1012            94         0.00\n#> 5           1016           1016            1014            65         0.00\n#> 6           1017           1017            1015            55         0.00\n#> 7           1019           1019            1016            69         0.00\n#> 8           1019           1019            1017            80         0.00\n#> 9           1019           1019            1016            89         0.00\n#> 10          1017           1017            1015            90         0.00\n#> 11          1018           1018            1016            90         0.00\n#> 12          1019           1019            1017            80         0.00\n#> 13          1020           1020            1018            75         0.00\n#> 14          1020           1020            1018            56         0.00\n#> 15          1021           1021            1018            85         0.00\n#> 16          1023           1023            1020            98         0.00\n#> 17          1023           1023            1020            98         0.00\n#> 18          1024           1024            1021            98         0.00\n#> 19          1025           1025            1022            78         0.00\n#> 20          1026           1026            1023            46         0.00\n#> 21          1025           1025            1023            40         0.00\n#> 22          1026           1026            1023            46         0.00\n#> 23          1027           1027            1024            65         0.00\n#> 24          1028           1028            1026            87         0.00\n#> 25          1029           1029            1026            95         0.00\n#> 26          1029           1029            1026            92         0.00\n#> 27          1030           1030            1028            67         0.00\n#> 28          1030           1030            1028            50         0.00\n#> 29          1030           1030            1027            45         0.00\n#> 30          1029           1029            1027            49         0.00\n#> 31          1030           1030            1027            69         0.00\n#> 32          1031           1031            1028            93         0.00\n#> 33          1031           1031            1028            97         0.00\n#> 34          1030           1030            1028            96         0.00\n#> 35          1030           1030            1027            73         0.00\n#> 36          1029           1029            1026            50         0.00\n#> 37          1027           1027            1025            41         0.00\n#> 38          1025           1025            1023            43         0.00\n#> 39          1025           1025            1022            70         0.00\n#> 40          1024           1024            1022            93         0.00\n#>                               weather all wind.speed wind.deg wind.gust\n#> 1          500, Rain, light rain, 10n  36       0.98      138      1.49\n#> 2          500, Rain, light rain, 10n  63       2.23      317      5.85\n#> 3   804, Clouds, overcast clouds, 04d  91       3.92      318      7.55\n#> 4   804, Clouds, overcast clouds, 04d 100       5.64      320     10.39\n#> 5   804, Clouds, overcast clouds, 04d 100       6.37      316      8.90\n#> 6   804, Clouds, overcast clouds, 04d 100       6.78      314      9.52\n#> 7   804, Clouds, overcast clouds, 04d  95       5.33      302      9.79\n#> 8     803, Clouds, broken clouds, 04n  63       3.94      287     10.72\n#> 9          500, Rain, light rain, 10n  68       3.51      262      9.92\n#> 10         500, Rain, light rain, 10n  99       4.77      269     10.80\n#> 11         500, Rain, light rain, 10d 100       4.05      300      7.60\n#> 12         500, Rain, light rain, 10d  99       2.53      323      3.65\n#> 13         500, Rain, light rain, 10d  99       2.62      301      3.11\n#> 14    803, Clouds, broken clouds, 04d  63       1.43      330      1.98\n#> 15         500, Rain, light rain, 10d  69       2.33      274      4.30\n#> 16         500, Rain, light rain, 10n  45       2.33      293      5.53\n#> 17 802, Clouds, scattered clouds, 03n  29       2.15      287      4.45\n#> 18       801, Clouds, few clouds, 02n  21       2.12      309      5.21\n#> 19 802, Clouds, scattered clouds, 03d  35       2.94      316      5.71\n#> 20         800, Clear, clear sky, 01d   4       3.71      324      4.98\n#> 21         800, Clear, clear sky, 01d   6       4.30      328      5.14\n#> 22         800, Clear, clear sky, 01d   5       5.09      331      6.11\n#> 23         800, Clear, clear sky, 01d   7       4.36      323      7.63\n#> 24 802, Clouds, scattered clouds, 03n  50       3.43      312     10.11\n#> 25 802, Clouds, scattered clouds, 03n  27       2.74      300     11.16\n#> 26         800, Clear, clear sky, 01n   0       2.68      297     10.59\n#> 27         800, Clear, clear sky, 01d   2       4.85      332      9.21\n#> 28    803, Clouds, broken clouds, 04d  59       4.20      333      5.99\n#> 29 802, Clouds, scattered clouds, 03d  36       4.28      328      5.92\n#> 30         800, Clear, clear sky, 01d   8       5.04      324      6.00\n#> 31         800, Clear, clear sky, 01d  10       4.26      315      6.76\n#> 32         800, Clear, clear sky, 01n   0       2.48      313      7.47\n#> 33         800, Clear, clear sky, 01n  10       1.98      332      3.86\n#> 34       801, Clouds, few clouds, 02n  12       1.45      339      1.46\n#> 35         800, Clear, clear sky, 01d   6       0.99      335      1.34\n#> 36         800, Clear, clear sky, 01d   1       1.23      344      1.44\n#> 37         800, Clear, clear sky, 01d   3       1.36       33      1.71\n#> 38         800, Clear, clear sky, 01d   4       2.16       22      2.04\n#> 39         800, Clear, clear sky, 01d   4       2.42       15      4.89\n#> 40         800, Clear, clear sky, 01n   5       1.65       58      1.66\n#>    visibility  pop   3h pod              dt_txt\n#> 1       10000 1.00 0.65   n 2023-05-23 00:00:00\n#> 2        8607 0.36 0.20   n 2023-05-23 03:00:00\n#> 3       10000 0.04   NA   d 2023-05-23 06:00:00\n#> 4       10000 0.00   NA   d 2023-05-23 09:00:00\n#> 5       10000 0.00   NA   d 2023-05-23 12:00:00\n#> 6       10000 0.00   NA   d 2023-05-23 15:00:00\n#> 7       10000 0.00   NA   d 2023-05-23 18:00:00\n#> 8       10000 0.00   NA   n 2023-05-23 21:00:00\n#> 9       10000 0.20 0.18   n 2023-05-24 00:00:00\n#> 10      10000 0.40 0.27   n 2023-05-24 03:00:00\n#> 11      10000 0.42 0.23   d 2023-05-24 06:00:00\n#> 12      10000 0.46 0.41   d 2023-05-24 09:00:00\n#> 13      10000 0.52 0.21   d 2023-05-24 12:00:00\n#> 14      10000 0.24   NA   d 2023-05-24 15:00:00\n#> 15      10000 0.20 0.13   d 2023-05-24 18:00:00\n#> 16      10000 0.20 0.10   n 2023-05-24 21:00:00\n#> 17      10000 0.00   NA   n 2023-05-25 00:00:00\n#> 18      10000 0.00   NA   n 2023-05-25 03:00:00\n#> 19      10000 0.00   NA   d 2023-05-25 06:00:00\n#> 20      10000 0.00   NA   d 2023-05-25 09:00:00\n#> 21      10000 0.00   NA   d 2023-05-25 12:00:00\n#> 22      10000 0.00   NA   d 2023-05-25 15:00:00\n#> 23      10000 0.00   NA   d 2023-05-25 18:00:00\n#> 24      10000 0.00   NA   n 2023-05-25 21:00:00\n#> 25      10000 0.00   NA   n 2023-05-26 00:00:00\n#> 26      10000 0.00   NA   n 2023-05-26 03:00:00\n#> 27      10000 0.00   NA   d 2023-05-26 06:00:00\n#> 28      10000 0.00   NA   d 2023-05-26 09:00:00\n#> 29      10000 0.00   NA   d 2023-05-26 12:00:00\n#> 30      10000 0.00   NA   d 2023-05-26 15:00:00\n#> 31      10000 0.00   NA   d 2023-05-26 18:00:00\n#> 32      10000 0.00   NA   n 2023-05-26 21:00:00\n#> 33      10000 0.00   NA   n 2023-05-27 00:00:00\n#> 34      10000 0.00   NA   n 2023-05-27 03:00:00\n#> 35      10000 0.00   NA   d 2023-05-27 06:00:00\n#> 36      10000 0.00   NA   d 2023-05-27 09:00:00\n#> 37      10000 0.00   NA   d 2023-05-27 12:00:00\n#> 38      10000 0.00   NA   d 2023-05-27 15:00:00\n#> 39      10000 0.00   NA   d 2023-05-27 18:00:00\n#> 40      10000 0.00   NA   n 2023-05-27 21:00:00\n#> \n#> $city\n#> $city$id\n#> [1] 2911298\n#> \n#> $city$name\n#> [1] \"Hamburg\"\n#> \n#> $city$coord\n#> $city$coord$lat\n#> [1] 53.55\n#> \n#> $city$coord$lon\n#> [1] 10\n#> \n#> \n#> $city$country\n#> [1] \"DE\"\n#> \n#> $city$population\n#> [1] 0\n#> \n#> $city$timezone\n#> [1] 7200\n#> \n#> $city$sunrise\n#> [1] 1684724969\n#> \n#> $city$sunset\n#> [1] 1684783445\n```\n:::\n\n```{.r .cell-code}\n# extract wind speed data from response\nwindspeed <- weather_extracted$list$wind$speed\nwindspeed  %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  num [1:40] 0.98 2.23 3.92 5.64 6.37 6.78 5.33 3.94 3.51 4.77 ...\n```\n:::\n\n```{.r .cell-code}\n# extract city name from response\ncity_name <- weather_extracted$city$name\n\n# hour list\nhours <- seq(0,120-1,3)\n\n# combine hour list and windspeed data\nwindspeed_tbl <- tibble(hours, windspeed)\nwindspeed_tbl %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 40\n#> Columns: 2\n#> $ hours     <dbl> 0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, …\n#> $ windspeed <dbl> 0.98, 2.23, 3.92, 5.64, 6.37, 6.78, 5.33, 3.94, 3.51, 4.77, …\n```\n:::\n:::\n\n\n## Visualization\n\n\n::: {.cell hash='02_data_acquisition_cache/html/unnamed-chunk-3_a5c66840f44c551fa5eb39c7622c943d'}\n\n```{.r .cell-code}\n# plot results\nwindspeed_tbl %>%\n  \n  # Set up x, y, fill\n  ggplot(aes(x = hours, y = windspeed)) +\n  \n  # Geometries\n  geom_col() + # Run up to here to get a stacked bar plot\n  \n  # Formatting\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \"m/s\")) +\n  labs(\n    title = glue(\"Wind speed in {city_name}\"),\n    subtitle = \"Forecast for the next 5 days in 3 hour intervals\",\n    x = \"Nr. of hours into the future\", \n    y = \"Wind speed in m/s\")\n```\n\n::: {.cell-output-display}\n![](02_data_acquisition_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n# Data Acquisition Challenge 2\n\n\n::: {.cell hash='02_data_acquisition_cache/html/unnamed-chunk-4_9c48c233b994f9a9778508434986254a'}\n\n```{.r .cell-code}\n# Challenge 2.2 ----\n\n# 1.0 LIBRARIES ----\n\nlibrary(tidyverse) # Main Package - Loads dplyr, purrr, etc.\nlibrary(rvest)     # HTML Hacking & Web Scraping\nlibrary(xopen)     # Quickly opening URLs\nlibrary(jsonlite)  # converts JSON files to R objects\nlibrary(glue)      # concatenate strings\nlibrary(stringi)   # character string/text processing\nlibrary(RSQLite)\nlibrary(httr)\nlibrary(xml2)\n\n# URL of shop\nbase_url <- \"https://www.rosebikes.com\"\n\n# Collecting info about bikes of MTB category\nmtb_url = glue(\"{base_url}/bikes/mtb\")\n\n# Open mtb url\n# xopen(mtb_url)\n\n# create table and extract href for model from html\nmtb_tbl <-  read_html(mtb_url) %>% \n  html_nodes(css = \".catalog-category-bikes__picture-wrapper\") %>% \n  map(xml_attrs) %>% \n  map(\"href\") %>% \n  unlist() %>% \n  as_tibble() %>% \n  rename(\"modelurl\" = value) %>%\n  # Add the domain, because we will get only the subdirectories\n  mutate(modelurl = glue(\"{base_url}{modelurl}\"))  %>%\n  \n  # Some categories are listed multiple times.\n  # We only need unique values\n  distinct(modelurl)\n\n# extract href for bikes of html for every url in mtb_tbl\nbike_tbl <- mtb_tbl$modelurl %>% \n  map(read_html) %>% \n  map(html_nodes, css = \".catalog-category-model__picture-link\") %>% \n  map(xml_attrs) \n\n# flatten nested lists\nbike_tbl <- unlist(bike_tbl, recursive=FALSE)\n\n# extract href from all bikes\nbike_tbl <- bike_tbl %>% \n  map(\"href\") %>% \n  unlist() %>% \n  as_tibble() %>% \n  rename(\"bikeurl\" = value) %>%\n  mutate(bikeurl = glue(\"{base_url}{bikeurl}\"))  %>%\n \n  # Some categories are listed multiple times.\n  # We only need unique values\n  distinct(bikeurl)\n\n\n# removing entries that give errors\nbike_tbl <- bike_tbl[-8,]\nbike_tbl <- bike_tbl[-52,]\n\n# save first entries of bike_tbl to variable for testing\n# bike_tbl <- bike_tbl[1:4,]  %>%   as_tibble()\n\nprice_tbl <- bike_tbl$bikeurl %>% \n  map(read_html) %>%\n  map(html_nodes, css = \".detail-price__wrapper > span\") %>% \n  map(xml_attrs)\n\n# flatten nested lists\nprice_tbl <- unlist(price_tbl, recursive=FALSE)\n\n# extract price from all entries\nprice_tbl <- price_tbl %>% \n  map(\"data-test\") %>% \n  unlist() %>% \n  as_tibble() %>% \n  rename(\"price\" = value)\n\n# remove dollar signs from price \nprice_tbl$price <- price_tbl$price %>% \n  str_remove_all(\"€\") %>% \n  str_remove_all(\"\\\\.\") %>% \n  as.numeric()/100\n\nname_tbl <- bike_tbl$bikeurl %>% \n  map(read_html) %>%\n  map(html_nodes, \".basic-headline__title\") %>% \n  map(xml_find_all, \".//text()\")\n\n# for loop through name_tbl to get only the text \nfor (i in 1:length(name_tbl)) {\n  name_tbl[[i]] <- as_list(name_tbl[[i]])[[1]]\n}\n\nname_tbl <- name_tbl %>% \n  unlist() %>% \n  as_tibble() %>% \n  rename(\"name\" = value)\n\n# combine rows from name_tbl and price_tbl\ncombi_tbl <- bind_cols(name_tbl, price_tbl)\n\n# preview first 10 rows\ncombi_tbl %>% head(10)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"price\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"COUNT SOLO 1\",\"2\":\"749\"},{\"1\":\"COUNT SOLO 1\",\"2\":\"749\"},{\"1\":\"COUNT SOLO 2\",\"2\":\"849\"},{\"1\":\"COUNT SOLO 2\",\"2\":\"849\"},{\"1\":\"COUNT SOLO 3\",\"2\":\"949\"},{\"1\":\"COUNT SOLO 3\",\"2\":\"949\"},{\"1\":\"PSYCHO PATH 1\",\"2\":\"1499\"},{\"1\":\"PSYCHO PATH 2\",\"2\":\"1799\"},{\"1\":\"PSYCHO PATH 2\",\"2\":\"1799\"},{\"1\":\"THRILL HILL 4\",\"2\":\"5499\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nNote the duplicates in the names are due to different color options which are not yet included.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}